---
title: "init"
author: "Nate Thomas"
date: "1/17/2022"
output: html_document
---

```{r}
library(tidyverse)
library(skedastic)
library(lubridate)
library(rvest)
library(readxl)
```

# Initalialization 

Source:
https://www.kaggle.com/vagifa/usa-commodity-prices?select=commodity-prices-2016.csv

```{r}
commodity <- read_csv('../DataSet/commodity prices/commodity-prices-2016.csv') %>% as_tibble()

df <- commodity %>% select(Date,`Industrial Inputs Price Index`) %>% 
  mutate(Date = as.Date(Date, format = "%m/%d/%Y"),
         Year = year(Date),
         Month = month(Date, abbr = FALSE),
         iipi = `Industrial Inputs Price Index`)

plot(df$Date,df$iipi)
```

## Time Series decomposition

```{r}
df.ts <- ts(df$iipi, start=c(1980,1), frequency = 12)
class(df.ts)

plot(aggregate(df.ts))
plot(decompose(df.ts))
```

## White Noise Process Confirmation

Criteria 1: autocorrelation
The Ljung-Box Q-statistic Test is used to confirm existence of autocorrelation within the commodity index.

```{r}
Box.test(df$iipi,lag=430)
```

H0: All autocorrelctions are zero (0)
Ha: at least one (1) autocorrelation is not zero (0)

The test indicates one can safely reject the null hypothesis (H0) - such that at least one (1) autocorrleation is not zero.

```{r}
acf(df$iipi, lag.max=430)
```

Based on the autocorrelation table displayed one can be confident that there exists autocorrelation to a great degree, across a large number of lag values.

Therefore one can safely conclude that this is not a white noise process.  The constant mean, and constant variance tests are performed as well for completeness.

First, informally observing the data to see if it is approximately normally distributed:

```{r}
df$iipi %>% hist()
```

The data does not appear, subjectively, to be normally distributed.  Further indicating this is not a white noise process.  This if formally confirmed using the Shapiro-Whilk Normality Test:

```{r}
shapiro.test(df$iipi)
```

H0: Series are normally distributed.
Ha: Series are not normally distributed.

The test indicates one can safely reject the null hypothesis (H0) - such that the data is not normally distributed.

As additional verification, a linear model is estimated of commodity index of interest as a function of series date.  

```{r}
slm <- lm(iipi~Date, data = df)
summary(slm)
```
The purpose of this reasoning is that if the slope estimate is shown to be statistically non-zero while as a special case, then one can be confident that the mean is not constant across time. This work acknowledges that if the test fails to confirm a non-zero value it does not indicate definitively that the data is white noise.  For example pure sine waves, when averaged across multiple periods, tend to a mean of zero, though sine wave are clearly not white noise.  The test above, shows clearly that the date is a linear function of time, and therefore the commodity index does not have a constant mean. 

Finally, White's Test for heteroscedasticity is performed. This is accomplished by creating a constant value linear model (such that the only estimated parameter is the intercept).  Using this model, and the understanding that the sum of the squared residuals are equivalent to the variance, one can then leverage White's Test on the model to confirm the constant variance criteria:

```{r}
white_lm(slm)
```

## First Difference White Noise Process

```{r}
df <- df %>% 
  mutate(iipi_dif = iipi - lag(iipi))
```

Criteria 1: autocorrelation
The Ljung-Box Q-statistic Test is used to confirm existence of autocorrelation within the commodity index first difference.

```{r}
Box.test(df$iipi_dif,lag=30)
```

H0: All autocorrelctions are zero (0)
Ha: at least one (1) autocorrelation is not zero (0)

The test indicates one can safely reject the null hypothesis (H0) - such that at least one (1) autocorrleation is not zero.

```{r}
acf(df %>% drop_na() %>% select(iipi_dif), lag.max=30)
```

Based on the autocorrelation table displayed one can be confident that there exists autocorrelation to a great degree, across a large number of lag values.

Therefore one can safely conclude that this is not a white noise process.  The constant mean, and constant variance tests are performed as well for completeness.

First, informally observing the data to see if it is approximately normally distributed:

```{r}
df$iipi %>% hist()
```

The data does not appear, subjectively, to be normally distributed.  Further indicating this is not a white noise process.  This if formally confirmed using the Shapiro-Whilk Normality Test:

```{r}
shapiro.test(df$iipi)
```

H0: Series are normally distributed.
Ha: Series are not normally distributed.

The test indicates one can safely reject the null hypothesis (H0) - such that the data is not normally distributed.

As additional verification, a linear model is estimated of commodity index of interest as a function of series date.  

```{r}
slm <- lm(iipi~Date, data = df)
summary(slm)
```
The purpose of this reasoning is that if the slope estimate is shown to be statistically non-zero while as a special case, then one can be confident that the mean is not constant across time. This work acknowledges that if the test fails to confirm a non-zero value it does not indicate definitively that the data is white noise.  For example pure sine waves, when averaged across multiple periods, tend to a mean of zero, though sine wave are clearly not white noise.  The test above, shows clearly that the date is a linear function of time, and therefore the commodity index does not have a constant mean. 

Finally, White's Test for heteroscedasticity is performed. This is accomplished by creating a constant value linear model (such that the only estimated parameter is the intercept).  Using this model, and the understanding that the sum of the squared residuals are equivalent to the variance, one can then leverage White's Test on the model to confirm the constant variance criteria:

```{r}
white_lm(slm)
```

# Predictors

## T-bill rate 

Source:
https://finance.yahoo.com/quote/%5EIRX/history?period1=317088000&period2=1453161600&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true

```{r}
tbill <- read_csv('../DataSet/commodity prices/1-tbill13week.csv') %>% as_tibble() %>%
  mutate(Date = as.Date(Date, format = "%m/%d/%Y"),
         Open = as.double(Open),
         High = as.double(High),
         Low = as.double(Low),
         Close = as.double(Close),
         AdjClose = as.double(`Adj Close`),
         Volume = as.double(Volume),
         Month = month(Date),
         Year = year(Date)) %>% 
  drop_na() %>%
  group_by(Year,Month) %>%
  summarise(Close = mean(Close))
  
plot(tbill$Close)
```

## GDP

https://fred.stlouisfed.org/series/BBKMGDP

```{r}
gdp <- read_xls('../DataSet/commodity prices/2-BBKMGDP.xls', sheet = 'Sheet1') %>% as_tibble() %>%
  mutate(Date = as.Date(observation_date, format = "%m/%d/%Y"),
         Month = month(Date),
         Year = year(Date)) %>%
  select(Year, Month, BBKMGDP)
gdp

plot(gdp$BBKMGDP)
```

## Inflation Rate, yearly

https://www.thebalance.com/u-s-inflation-rate-history-by-year-and-forecast-3306093#:~:text=U.S.%20Inflation%20Rate%20History%20and%20Forecast%20%20,FFR%20midpoint%20projection%20%2037%20more%20rows%20

```{r}
inflation_yearly <- read_xlsx('../DataSet/commodity prices/3-annual-inflation-rate.xlsx', sheet = 'Sheet1') %>%
  as_tibble() %>%
  mutate(Year = as.double(Year),
         Inf_Rate = as.double(`Inflation Rate YOY4`)) %>%
  select(Year, Inf_Rate) %>%
  drop_na()
inflation_yearly
```

## Inflation Rate, Annualized Monthly value

https://www.multpl.com/inflation/table/by-month

```{r}
inflation_monthy <- read_xlsx('../DataSet/commodity prices/4-monthly-inflation.xlsx', sheet = 'Sheet1') %>%
  as_tibble() %>%
  mutate(Date = as.Date(Date),
         Year = year(Date),
         Month = month(Date),
         Rate = as.double(Value)*100) %>%
  select(Date, Month, Year, Rate) %>%
  drop_na()
inflation_monthy

plot(inflation_monthy$Rate)
```

## 




















