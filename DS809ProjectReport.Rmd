---
title: "DS809 - Industrial Inputs Price Index"
author: "Nate Thomas"
date: "`r Sys.Date()`"
output: word_document
---

```{r echo = FALSE, message=FALSE, results='hide'}
load('.RData')
library(dplyr)
library(car)
library(skedastic)
library(kableExtra)
library(TSA)
library(tseries)
```

# Introduction and Overview

This project endeavors to provide example implementation of the various methods outlined in DS809 Time Series to model the commodity Industrial Inputs Price Index from 1980 through 2016. The Industrial Input Price Index, hereafter IIPI, is provided by the IMF in the IMF Primary Commodity Prices Report (*IMF Primary Commodity Prices*, 2021). As a break-out index it does not include energy commodities (Coal, Natural Gas, Spot Crude, Propane) but does include Agricultural raw materials (Cotton, Hides, Rubber, Timber, Wool), metals (Aluminum, Cobalt, Copper, Iron Ore, Lead, Molybdenum, Nickel, Tin, Uranium, and Zinc), and precious metals (Gold, Palladium, Platinum, Silver). The commodity prices used are weighted averages representative of the global market, corrected for volume, as determined by the largest import markets for the commodity constituents. The purpose of the index is to provide a relative value indicator (here indexed to 2005 = 100) for assessment of the overall commodities market in question.

The first task performed for this project was to create the time series dataset using *Predictive Dynamics in Commodity Prices* (Gargano & Timmerman, 2012) as a loose framework with select variables therein. These  were collected and organized for subsequent analysis.

A. IIPI (Aliyev, 2020), Dependent Variable and Date index. tags: **IIPI, Date_form**. This dataset defined the time frame of interest (1980-02-01 to 2016-02-01). All other datasets were selected contingent upon their inclusion of this timeframe.

Plot of IIPI as a function of time:

```{r echo = FALSE}
p1
```

B. 13 Week Treasury Bill (*US Trade - Statistics & Facts*, 2021), Independent Variable. tag: **TBillClose**.

C. US Real GDP (*Real Gross Domestic Product (GDPC1)* \| *FRED* \| *St. Louis Fed*, n.d.). tag: **GDP**

D. US Inflation (*US Inflation Rate by Month*, n.d.). tag: **InfRate**

E. Industrial Production (*Industrial Production: Total Index (INDPRO)*\| *FRED* \| *St. Louis Fed*, n.d.). tag: **IndPro**

F. Money Supply M2, (*BOARD OF GOVERNORS of the FEDERAL RESERVE SYSTEM*, 2021). tag: **M2**

G. US Unemployment, (*Employment and Unemployment*). tag: **UnempRate**

H. Recession, (*Dates of U.S. Recessions as Inferred by GDP-based Recession Indicator*). tag : **Recession**

The index is indicative of global market but this project uses United States specific independent variable values as analog facsimiles for global variables. This is a recognized short-hand performed here to allow for timely and comprehensive analysis, in the absence of the global datasets for use. United States' trade accounts for the largest percentage (13.5%) of total import trade worldwide (*US Trade - Statistics & Facts*, 2021), and therefore while this is not perfect it is used for the purposes defined here.

The first rows of the dataset are displayed after collection, cleaning, and being joined according to date:

```{r echo = FALSE, size = "tiny"}
head(df.ts.modelset)
```

## Visual Inspection of the Data

The data was decomposed using native R *stats* package to visually assess the potential time series affects of IIPI.

```{r echo = FALSE}
plot(decompose(df.ts))
```

## Model assessment strategy

A time series model is as best it can be when the residuals are shown to be a completely white noise process.  This implies that the model has capture all non-stochastic movement of the series with respect to the model independent variables.  Therefore, in this project many models will be developed then immediately abandonded due to a failure to provide white noise residuals.

## Excluding White Noise Criteria

The first task when considering a dataset for time series analysis is to confirm that the variable of interest and its first difference are not white noise processes (WNP).  If they are WNPs then the goal of predicting and assessing the series is complete.  Nothing else can be assessed beyond this point as all movement of the data is presumed to be fully stochastic.  A WNP is defined by three (3) criteria, all of which must be met to conclude the data is WNP.

1) Constant Mean: $E(IIPI_t) = \mu$ for all $t$

2) Constant Variance: $Var(Y_t) = \sigma^2$ for all $t$

3) Autocorrelation is not found at any lags: $\rho_k = 0$ for all $k\geq1$

To show then that a series is not a WNP one must simply prove at least one of these constraints to be false.

This is performed a number of times throughout this project.

### IIPI White Noise Rejection

The Box-Pierce Test is used to confirm the existence of autocorrelation up to a defined maximum lag value (here 36 months):

$H_0$: All autocorrelctions are zero (0), $H_a$: At least one (1) autocorrelation is not zero (0) 

```{r echo = FALSE}
Box.test(df$IIPI,lag=36)
```

The test indicates one can safely reject the null hypothesis ($H_0$) - such that at least one (1) autocorrleation is not zero.

This is also seen through visual inspection of the ACF chart:

```{r echo = FALSE}
acf(df$IIPI, lag.max=36)
```

Based on the autocorrelation chart displayed one can be very confident that there exists autocorrelation to a great degree, across the three years of lag values shown.  The series is non-stationary as seen be by the slow decay in autocorrelation as lag values increase.

The series is not a white noise process.

### IIPI First Difference White Noise Process Rejection

Again, the Box-Pierce Q-statistic Test is used to confirm the existence of autocorrelation up to a defined maximum lag value (here 36 months) for also the first difference:

$H_0$: All autocorrelctions are zero (0), $H_a$: At least one (1) autocorrelation is not zero (0) 

```{r echo = FALSE}
Box.test(df$IIPI_dif,lag=36)
```

The test indicates one can safely reject the null hypothesis (H0) - such that at least one (1) autocorrleation is not zero in the first difference series.

This is also seen through visual inspection of the ACF chart:

```{r echo = FALSE}
acf(df$IIPI_dif, lag.max=36)
```

The first difference of the data is stationary as can be seen in the fast decay in autocorrelation, however, it is not white noise due to the autocorrelations mostly at lags 1, 2, 24, 28, 29, and 36.

# Regression Models

## Initial Regression, with all variables

A multiple linear regression is estimated:

$IIPI(`r names(mlm.ts$coefficients)[2:length(names(mlm.ts$coefficients))] %>% paste(sep = " ")`) = \hat{\beta}_0 + \hat{\beta}_1 TBillCLose + \hat{\beta}_2 GDP  + \hat{\beta}_3 InfRate + \hat{\beta}_4 IndPro + \hat{\beta}_5 M2 + \hat{\beta}_6 UnempRate + \hat{\beta}_7 Recession + \epsilon_t$

The summary and analysis of these coefficients is not performed here, due to failure of all residual assumptions, as follows.

### Residual Assumptions

Before proceeding with model interpretation the regression residual assumptions must be verified:

1. $E(\epsilon_t|X_{1t},...,X_{kt}) = 0$ (zero mean assumption)

2. $Var(\epsilon_t|X_{1t},...,X_{kt}) = Var(\epsilon_t) = \sigma^2$ for all $t$ (constant variance assumption)

3. $(\epsilon_t|X_{1t},...,X_{kt}) ~N(0,\sigma^2)$ for all $t$ (normality assumption)

4. $Cov((\epsilon_t,\epsilon_h|X_{1t},...,X_{kt}) = 0$ for $t \neq h$ (error terms are not autocorrelated along the time dimension)

#### Residual Normality

The Shapiro-Wilk Test is performed to verify residual normality:

```{r}
shapiro.test(rstandard(mlm.ts))
```

$H_0$: Series are normally distributed. vs. $H_a$: Series are not normally distributed

The resulting p-value is less than 0.05, therefore $H_0$ is rejected, and one must conclude that the series are not normally distributed.

The residuals are therefore not white noise. There other assumption verfications are performed to ensure fully rigorous analysis, however are technically unnecessary given this result. 

#### Residual Autocorrelation

The Box-Pierce test is performed to identify if the residuals are autocorrelated: 

```{r}
Box.test(rstandard(mlm.ts), lag=36)

```

$H_0$: $\rho_1=\rho_2=\rho_k=0$ (all autocorrelations are zero). vs. $H_a$: at least one $\rho_k \neq 0$ (at least one autocorrealtion is not zero)

The residuals are highly autocorrelated and non-stationary.  This model can be improved greatly, as there still appears to be time series information not captured in this model.

The resulting p-value is less than 0.05, therefore $H_0$ is rejected, and one must conclude that at least one lag value returns a statistically significant autocorrelation, as seen below:

```{r}
acf(rstandard(mlm.ts))
```

#### Constant Variance

White's test for Heteroscedasticity is performed to confirm constant variance:

```{r}
white_lm(mlm.ts, interactions = TRUE)
```

$H_0$: There is no Heteroscedasticity (constant variance) vs. $H_a$: There is Heteroscedasticity (no constant variance)

The resulting p-value is less than 0.05, therefore $H_0$ is rejected, and one must conclude that there exists non-constant variance within the residuals.

### Initial Regression, ResidualSummary

All the residual assumptions failed for this initial regression.  Subsequent methods and analysis are required to evaluate the complexities presented by this dataset.

In conjunction with resolving the residual assumptions, the variance inflation factors are computed to identify any multi-collinearities.  

```{r echo = FALSE}
vif(mlm.ts)
```

Ten (10) is the typically accepted subjective value at which a variance inflation factor is considered high.  GDP, IndPro and M2 all exceed this limit.

A new linear model is therefore estimated without GDP, which has the highest variance inflation factor.

## Regression without GDP

The following multiple linear regression model is estimated:

$IIPI(`r names(mlm.ts1$coefficients)[2:length(names(mlm.ts1$coefficients))] %>% paste(sep = " ")`) = \hat{\beta}_0 + \hat{\beta}_1 TBillCLose + \hat{\beta}_3 InfRate + \hat{\beta}_4 IndPro + \hat{\beta}_5 M2 + \hat{\beta}_6 UnempRate + \hat{\beta}_7 Recession + \epsilon_t$

, which results in the following estimated $\hat{\beta}$ values:

```{r echo = FALSE}
mlm.ts1$coefficients
```

Again, summary and analysis of these coefficients is not performed here due to failure of all residual assumptions, as follows.

Now, to continue improving the model by reducing inter- independent variable multicollinarity, the variance inflation factors are again computed before proceeding to assess the residual assumptions.

```{r echo = FALSE}
vif(mlm.ts1)
```

IndPro exceeds the limit and is removed.

A new linear model is therefore estimated without GDP and IndPro.

## Regression without GDP and IndPro

The following multiple linear regression model is estimated:

$IIPI(`r names(mlm.ts2$coefficients)[2:length(names(mlm.ts2$coefficients))] %>% paste(sep = " ")`) = \hat{\beta}_0 + \hat{\beta}_1 TBillCLose + \hat{\beta}_3 InfRate + \hat{\beta}_5 M2 + \hat{\beta}_6 UnempRate + \hat{\beta}_7 Recession + \epsilon_t$

, which results in the following estimated $\hat{\hat{\beta}}$ values:

```{r echo = FALSE}
mlm.ts2$coefficients
```

### Model Fit Plot

```{r echo = FALSE}
#Plotting the confidence and prediction intervals
predint =  predict(mlm.ts2,df.ts.modelset,interval="prediction")
confint =  predict(mlm.ts2,df.ts.modelset,interval="confidence")
predlower = predint[,2]
predupper = predint[,3]
conflower = confint[,2]
confupper = confint[,3]

plot(df.ts.modelset$IIPI, col="lightgray")
lines(predint[,1],col="red")
abline(v=424,col="blue", lty=5)
lines(predlower,col="orange")
lines(predupper,col="orange")
lines(conflower,col="blue")
lines(confupper,col="blue")
```

Variance inflation factors are computed again to identify multi-collinearities.  

```{r echo = FALSE}
vif(mlm.ts2)
```

The removal of GDP and IndPro in the regression model has resolved the multicollinearity issues, as can be seen by the fact that all VIF values now less than ten (10). 

Therefore the residual assumptions are tested, as follows.

### Residual Assumptions

#### Residual Normality

The Shapiro-Wilk Test is performed to verify residual normality:

```{r}
shapiro.test(rstandard(mlm.ts2))
```

$H_0$: Series are normally distributed. vs. $H_a$: Series are not normally distributed

The resulting p-value is less than 0.05, therefore $H_0$ is rejected, and one must conclude that the series are not normally distributed.

#### Residual Autocorrelation

The Box-Pierce test is performed to identify if the residuals are autocorrelated: 

```{r}
Box.test(rstandard(mlm.ts2), lag=36)
```

$H_0$: $\rho_1=\rho_2=\rho_k=0$ (all autocorrelations are zero). vs. $H_a$: at least one $\rho_k \neq 0$ (at least one autocorrealtion is not zero)

The resulting p-value is less than 0.05, therefore $H_0$ is rejected, and one must conclude that at least one lag value returns a statistically significant autocorrelation.

The following is the autcorrelation function (ACF) chart:

```{r}
acf(rstandard(mlm.ts2))
```

This slow decay of the autocorrelations indicate this is a non-stationary process.  This model does not capture all the time series related information. Further model improvement is necessary. 

#### Constant Variance

White's test for Heteroscedasticity is performed to confirm constant variance:

```{r}
white_lm(mlm.ts2, interactions = TRUE)
```

$H_0$: There is no Heteroscedasticity (constant variance) vs. $H_a$: There is Heteroscedasticity (no constant variance)

The resulting p-value is less than 0.05, therefore $H_0$ is rejected, and one must conclude that there exists non-constant variance within the series.

### Residual Summary

All the residual assumptions failed for the regression excluding GDP and IndPro.  

### Model Summary

Despite the failure of the residual assumptions a brief summary is provided of the coefficient interpretations, if the assumptions had been met for completeness.

```{r}
summary(mlm.ts2)
```

All following statements should be seen within the context of "if the residual assumptions were met, then..."

- The expected value of IIPI if all independent variables are zero is approximately zero (0) as the p-value for the intercept is not significant.   

- The expected change in the value of IIPI, ceteris paribus, for each unit change in treasury bill close is approximately zero (0) as the p-value for this coefficient is not significant.

- The expected change in the value of IIPI, ceteris paribus, for each unit change in inflation is `r mlm.ts2$coefficients[3]`

- The expected change in the value of IIPI, ceteris paribus, for each unit change in money supply is `r mlm.ts2$coefficients[4]`

- The expected change in the value of IIPI, ceteris paribus, for each unit change in unemployment is `r mlm.ts2$coefficients[5]`

- The expected change in the value of IIPI, ceteris paribus, if the market cycle is indicated as being in recession is `r mlm.ts2$coefficients[6]`


## Regression without GDP and IndPro, and with variance stabilizing function, natural log.

The following multiple linear regression model is estimated:

$ln(IIPI)(`r names(mlm.ts3$coefficients)[2:length(names(mlm.ts3$coefficients))] %>% paste(sep = " ")`) = \hat{\beta}_0 + \hat{\beta}_1 TBillCLose + \hat{\beta}_3 InfRate + \hat{\beta}_5 M2 + \hat{\beta}_6 UnempRate + \hat{\beta}_7 Recession + \epsilon_t$

### Model Fit Plot

```{r echo = FALSE}
#Plotting the confidence and prediction intervals
predint =  exp(predict(mlm.ts3,df.ts.modelset,interval="prediction"))
confint =  exp(predict(mlm.ts3,df.ts.modelset,interval="confidence"))
predlower = predint[,2]
predupper = predint[,3]
conflower = confint[,2]
confupper = confint[,3]

plot(df.ts.modelset$IIPI, col="lightgray")
lines(predint[,1],col="red")
abline(v=424,col="blue", lty=5)
lines(predlower,col="orange")
lines(predupper,col="orange")
lines(conflower,col="blue")
lines(confupper,col="blue")
```

#### Constant Variance

White's test for Heteroscedasticity is performed to confirm constant variance:

```{r}
white_lm(mlm.ts3, interactions = TRUE)
```

$H_0$: There is no Heteroscedasticity (constant variance) vs. $H_a$: There is Heteroscedasticity (no constant variance)

The resulting p-value is less than 0.05, therefore $H_0$ is rejected, and one must conclude that there exists non-constant variance within the series. Applying the natural log conversion did not resolve the issue with heteroskedasticity.

### Model Summary

The interpretability of the model coefficients within the context of actual units of IIPI has been obscured by the natural log function and the residual assumptions failed  - therefore the model coefficients are not interpreted here.


## Regression without GDP and IndPro, and with variance stabilizing function, square-root

The following multiple linear regression model is estimated:

$\sqrt{IIPI(`r names(mlm.ts4$coefficients)[2:length(names(mlm.ts4$coefficients))] %>% paste(sep = " ")`)} = \hat{\beta}_0 + \hat{\beta}_1 TBillCLose + \hat{\beta}_3 InfRate + \hat{\beta}_5 M2 + \hat{\beta}_6 UnempRate + \hat{\beta}_7 Recession + \epsilon_t$

### Model Fit Summary

```{r echo = FALSE}
#Plotting the confidence and prediction intervals
predint =  (predict(mlm.ts4,df.ts.modelset,interval="prediction"))^2
confint =  (predict(mlm.ts4,df.ts.modelset,interval="confidence"))^2
predlower = predint[,2]
predupper = predint[,3]
conflower = confint[,2]
confupper = confint[,3]

plot(df.ts.modelset$IIPI, col="lightgray")
lines(predint[,1],col="red")
abline(v=424,col="blue", lty=5)
lines(predlower,col="orange")
lines(predupper,col="orange")
lines(conflower,col="blue")
lines(confupper,col="blue")
```

### Residual Assumptions

#### Constant Variance

White's test for Heteroscedasticity is performed to confirm constant variance:

```{r}
white_lm(mlm.ts4, interactions = TRUE)
```

$H_0$: There is no Heteroscedasticity (constant variance) vs. $H_a$: There is Heteroscedasticity (no constant variance)

The resulting p-value is less than 0.05, therefore $H_0$ is rejected, and one must conclude that there exists non-constant variance within the series. Applying the square-root conversion did not resolve the issue with heteroskedasticity.

# Deterministic Models

## Indicator Variable Regression Model with Trend

A month indicator variable linear regression is estimated, correcting for linear time trend:

$IIPI(`r names(dlm.ts1$coefficients)[2:4] %>% paste(sep = " ")`,...,`r names(dlm.ts1$coefficients)[13]`) = \hat{\beta}_0 + \hat{\beta}_1 t + \hat{\beta}_{1+i}\sum_{i=1}^{12} Month_i +  \epsilon_t$

### Model Fit Plot

```{r echo = FALSE}
#Plotting the confidence and prediction intervals
predint =  predict(dlm.ts1,df.ts.modelset.deterministic,interval="prediction")
confint =  predict(dlm.ts1,df.ts.modelset.deterministic,interval="confidence")
predlower = predint[,2]
predupper = predint[,3]
conflower = confint[,2]
confupper = confint[,3]

plot(df.ts.modelset.deterministic$IIPI, col="lightgray")
lines(predint[,1],col="red")
abline(v=424,col="blue", lty=5)
lines(predlower,col="orange")
lines(predupper,col="orange")
lines(conflower,col="blue")
lines(confupper,col="blue")
```

### Residual Assumptions

#### Residual Autocorrelation

The Box-Pierce test is performed to identify if the residuals are autocorrelated: 

```{r}
Box.test(rstandard(dlm.ts1))
```

$H_0$ is rejected.  Autocorrelation is present in the series.

```{r}
acf(rstandard(dlm.ts1))
```

Again, this residuals are highly autoccorrelated and non-stationary. Other methods will be investigated to determine a model which results in stationary white noise residuals.

#### Residual Normality

The Shapiro-Wilk Test is performed to verify residual normality:

```{r}
hist(rstandard(dlm.ts1))
shapiro.test(rstandard(dlm.ts1))
```

As can be seen, and is confirmed by the Shapiro-Wilk test, the residuals of the model are not normally distributed.

#### Constant Variance

White's test for Heteroscedasticity is performed to confirm constant variance:

```{r}
white_lm(dlm.ts1, interactions = TRUE)
```

$H_0$ is accepted, therefore allowing one to conclude that the residuals have constant variance.

### Model Summary

```{r}
summary(dlm.ts1)
```

If the residual assumptions were confirmed then one could interpret the t-test p-values as indicating that there did not appear to be any monthly trends.

###################

## Polynomial Model, k=5

A polynomial linear regression is estimated:

$$IIPI(t) = \hat{\beta}_0 + \hat{\beta}_1 t + \hat{\beta}_{2}t^2 + ...+ \hat{\beta}_{k}t^k + \epsilon_t$$

The highest order model which does not result in an increase in the adjusted R-squared was found using the following function:

```{r eval=FALSE}
kfind <- function(d) {
  r2adjust <- 0
  for (k in 1:30){
    lm_summary_temp <- lm(IIPI~poly(index,k),data = d) %>% summary()
    print(k)
    print(lm_summary_temp$adj.r.squared)
    if (lm_summary_temp$adj.r.squared>r2adjust){
      k_opt <- k
    }
    if(lm_summary_temp$adj.r.squared<=r2adjust){
      break
    }
    r2adjust<-lm_summary_temp$adj.r.squared
  }
  return(k_opt)
}
```

This occurs at:

$$k = 5$$

### Model Fit Plot

```{r echo = FALSE}
#Plotting the confidence and prediction intervals
predint =  predict(dlm.ts2,df.ts.modelset.deterministic,interval="prediction")
confint =  predict(dlm.ts2,df.ts.modelset.deterministic,interval="confidence")
predlower = predint[,2]
predupper = predint[,3]
conflower = confint[,2]
confupper = confint[,3]

plot(df.ts.modelset.deterministic$IIPI, col="lightgray")
lines(predint[,1],col="red")
abline(v=424,col="blue", lty=5)
lines(predlower,col="orange")
lines(predupper,col="orange")
lines(conflower,col="blue")
lines(confupper,col="blue")
```

### Residual Assumptions

#### Residual Autocorrelation

The Box-Pierce test is performed to identify if the residuals are autocorrelated: 

```{r}
Box.test(rstandard(dlm.ts2))
```

$H_0$ is rejected.  Autocorrelation is present in the series.

```{r}
acf(rstandard(dlm.ts2))
```

Again, the residuals are highly autoccorrelated and non-stationary.  More work is required to improve the model. There is more inherent information in the time-series component that is not captured by this model.

#### Residual Normality

The Shapiro-Wilk Test is performed to verify residual normality:

```{r}
hist(rstandard(dlm.ts2))
shapiro.test(rstandard(dlm.ts2))
```

As can be seen, and is confirmed by the Shapiro-Wilk test, the residuals of the model are not normally distributed.

#### Constant Variance

White's test for Heteroscedasticity is performed to confirm constant variance:

```{r}
white_lm(dlm.ts2, interactions = TRUE)
```

Finally the residuals are also seen to display constant variance.

### Model Summary

```{r}
summary(dlm.ts2)
```

If the residual assumptions were found to be true one could interpret the coefficients as follows. All p-values indicate significance (though the residual distribution assumptions fail making interpretation dubious).

- The expected value of IIPI at time zero (Feb, 1980) is `r dlm.ts2$coefficients[1]`.   

- The expected change in the value of IIPI, ceteris paribus, for each unit change the time index is `r dlm.ts2$coefficients[2]`.

- The expected change in the value of IIPI, ceteris paribus, for each unit change the time index squared is `r dlm.ts2$coefficients[3]`.

- The expected change in the value of IIPI, ceteris paribus, for each unit change the time index cubed is `r dlm.ts2$coefficients[4]`.

- The expected change in the value of IIPI, ceteris paribus, for each unit change the time index raised to the fourth power is `r dlm.ts2$coefficients[5]`.

- The expected change in the value of IIPI, ceteris paribus, for each unit change the time index raised to the fifth power is `r dlm.ts2$coefficients[6]`.

## Harmonic Model

A de-trended harmonic model is estimated:

$IIPI(t) = \hat{\beta}_0 + \hat{\beta}_1 t + \sum{\hat{\alpha_i}sin(\frac{2\pi i}{s}t)} + \sum{\hat{\beta}_icos(\frac{2\pi i}{s}t)} + \epsilon_t$

### Periodogram

To determine the harmonics, a periodogram is reviewed:

```{r echo = FALSE}
periodogram(detrend$residuals)
```

Unfortunately, the periodogram has such high density at lower harmonics the number of peaks is not readily countable. 

Therefore, a function is created to create the model using the top 100 harmonics.  While this expected to be an over estimate, the model harmonic terms are subsequently removed by evaluating that the sine and cosine pair are both not significant in the model 

```{r eval = FALSE}
listgen <- function(t,j,h,s){
  sin_df <- tibble(rep(NA,length(t)))
  cos_df <- tibble(rep(NA,length(t)))
  for (i in 1:j){
    sinx = sin(2*pi*t*h[i]/length(s))
    cosx = cos(2*pi*t*h[i]/length(s))
    sin_df <- cbind(sin_df,sinx)
    cos_df <- cbind(cos_df,cosx)
  }
  colnames(sin_df) = c('temp',paste('sin',1:j,sep=""))
  colnames(cos_df) = c('temp',paste('cos',1:j,sep=""))
  sin_df <- sin_df %>% select(-temp)
  cos_df <- cos_df %>% select(-temp)
  df <- cbind(sin_df,cos_df)
  return(df)
}

trigf <- listgen(df.ts.modelset.harmonic$index,length(harmonics),harmonics,df.ts.modelset.harmonic$IIPI)
```

```{r echo = FALSE}
head(trigf)
```

A linear model is estimated with all 200 harmonic terms (one sine and one cosine per harmonic), and those harmonics which have an insignificant (p is greater than 0.05) coefficient terms are displayed:

```{r echo = FALSE}
which(dlm.ts3.summary$coefficients[,'Pr(>|t|)']>0.05) %>% names()
```

Then those terms which have the same harmonic numeric indicator are removed - ensuring that the full harmonic (both terms are not significant):

```{r eval=FALSE}
termsforassessment <- which(dlm.ts3.summary$coefficients[,'Pr(>|t|)']>0.05) %>% names() %>% tibble()
sins <- termsforassessment %>% filter(str_detect(.,'sin'))
coss <- termsforassessment %>% filter(str_detect(.,'cos'))

remove <- intersect(sins$`.` %>% str_extract(.,"(\\d+)"), coss$`.` %>% str_extract(.,"(\\d+)"))

termsforassessment <- termsforassessment$`.`
termsforremoval1 <- termsforassessment[termsforassessment %in% paste('sin',remove ,sep='')]
termsforremoval2 <-termsforassessment[termsforassessment %in% paste('cos',remove ,sep='')]
termsforremoval <- cbind(termsforremoval1,termsforremoval2) %>% c()
termsforremoval
```

The names of the terms are saved to a vector and subsequently removed from the model:

```{r echo=FALSE}
dlm.ts4.summary
```

### Model Fit Plot

This initial model with 85 harmonics significantly overfits the data.

```{r echo = FALSE}
#Plotting the confidence and prediction intervals
predint =  predict(dlm.ts4,df.ts.modelset.harmonic,interval="prediction")
confint =  predict(dlm.ts4,df.ts.modelset.harmonic,interval="confidence")
predlower = predint[,2]
predupper = predint[,3]
conflower = confint[,2]
confupper = confint[,3]

plot(df.ts.modelset.harmonic$IIPI, col="lightgray")
lines(predint[,1],col="red")
abline(v=424,col="blue", lty=5)
lines(predlower,col="orange")
lines(predupper,col="orange")
lines(conflower,col="blue")
lines(confupper,col="blue")
```

So as alterative the model is incrementaly increased in terms, until the adjusted R-squared does not increase. 

```{r eval = FALSE}
harmiclmgen <- function(d,harmonics){
  t <- d$index
  j <- length(harmonics)
  h = harmonics
  r2adj <- list()
  for (ix in 1:j) {
    sin_df <- tibble(rep(NA,length(t)))
    cos_df <- tibble(rep(NA,length(t)))
      for (i in 1:ix){
        sinx = sin(2*pi*t*h[i]/length(t))
        cosx = cos(2*pi*t*h[i]/length(t))
        sin_df <- cbind(sin_df,sinx)
        cos_df <- cbind(cos_df,cosx)
      }
    colnames(sin_df) = c('temp',paste('sin',h[1:ix],sep=""))
    colnames(cos_df) = c('temp',paste('cos',h[1:ix],sep=""))
    sin_df <- sin_df %>% select(-temp)
    cos_df <- cos_df %>% select(-temp)
    df <- d %>% cbind(cos_df) %>% cbind(sin_df)  
    df.train <-  df[1:423,]
    lms <- lm(IIPI~., data = df.train)
    lms_sum <- summary(lms)
    r2adj[ix] <- lms_sum$adj.r.squared
  }
  return(list(r2adj = r2adj,numh = 1:ix))
}

dlm.ts5 <- harmiclmgen(df.ts.modelset.deterministic %>% select(-Month),harmonics)

dlm.ts.r2tlb <- tibble(numh = dlm.ts5$numh %>% unlist(), 
r2adj = dlm.ts5$r2adj %>% unlist(), bic = dlm.ts5$bic %>% unlist(), aic = dlm.ts5$aic %>% unlist())
```


The three in-sample metrics are displayed as a function of increaing harmonics:

```{r echo = FALSE}
plot(dlm.ts.r2tlb$r2adj)
plot(dlm.ts.r2tlb$bic)
plot(dlm.ts.r2tlb$aic)
```

Using BIC for in-sample model assessment, 65 sine and cosine pairs appears to be possibly optimal.

```{r}
dlm.ts.r2tlb$bic %>% which.min()
```

```{r echo = FALSE}
#Plotting the confidence and prediction intervals
predint =  predict(dlm.ts7,df.ts.modelset.harmonic2,interval="prediction")
confint =  predict(dlm.ts7,df.ts.modelset.harmonic2,interval="confidence")
predlower = predint[,2]
predupper = predint[,3]
conflower = confint[,2]
confupper = confint[,3]

plot(df.ts.modelset.harmonic2$IIPI, col="lightgray")
lines(predint[,1],col="red")
abline(v=424,col="blue", lty=5)
lines(predlower,col="orange")
lines(predupper,col="orange")
lines(conflower,col="blue")
lines(confupper,col="blue")
```

```{r echo = FALSE}
#Plotting the confidence and prediction intervals
predint =  predict(dlm.ts7,df.ts.modelset.harmonic2[400:433,],interval="prediction")
confint =  predict(dlm.ts7,df.ts.modelset.harmonic2[400:433,],interval="confidence")
predlower = predint[,2]
predupper = predint[,3]
conflower = confint[,2]
confupper = confint[,3]

plot(df.ts.modelset.harmonic2$IIPI[400:433], col="lightgray",ylim = c(50,170))
lines(predint[,1],col="red")
abline(v=24,col="blue", lty=5)
lines(predlower,col="orange")
lines(predupper,col="orange")
lines(conflower,col="blue")
lines(confupper,col="blue")
```

However, this appears to still overfit the data, therefore instead the visual elbow method is used for adjusted R-squared metric.

Taking 15 harmonics before the reduced rate of increase in adjusted R-squared a linear model is estimated:

```{r}
dlm.ts6.summary
```

Only two trigonomtric terms (not of the same harmonic) are not significant.

```{r echo = FALSE}
which(dlm.ts6.summary$coefficients[,'Pr(>|t|)']>0.05) %>% names()
```
Therefore, this model is used going forward for comparison.

```{r echo = FALSE}
#Plotting the confidence and prediction intervals
predint =  predict(dlm.ts6,df.ts.modelset.harmonic1,interval="prediction")
confint =  predict(dlm.ts6,df.ts.modelset.harmonic1,interval="confidence")
predlower = predint[,2]
predupper = predint[,3]
conflower = confint[,2]
confupper = confint[,3]

plot(df.ts.modelset.harmonic1$IIPI, col="lightgray")
lines(predint[,1],col="red")
abline(v=424,col="blue", lty=5)
lines(predlower,col="orange")
lines(predupper,col="orange")
lines(conflower,col="blue")
lines(confupper,col="blue")
```

```{r echo = FALSE}
#Plotting the confidence and prediction intervals
predint =  predict(dlm.ts6,df.ts.modelset.harmonic1[400:433,],interval="prediction")
confint =  predict(dlm.ts6,df.ts.modelset.harmonic1[400:433,],interval="confidence")
predlower = predint[,2]
predupper = predint[,3]
conflower = confint[,2]
confupper = confint[,3]

plot(df.ts.modelset.harmonic1$IIPI[400:433], col="lightgray")
lines(predint[,1],col="red")
abline(v=24,col="blue", lty=5)
lines(predlower,col="orange")
lines(predupper,col="orange")
lines(conflower,col="blue")
lines(confupper,col="blue")
```

### Residual Assumptions (model with 15 harmonics)

#### Residual Autocorrelation

The Box-Pierce test is performed to identify if the residuals are autocorrelated for the model with 15 harmonics: 

```{r}
Box.test(rstandard(dlm.ts6), lag=36)
```

$H_0$: $\rho_1=\rho_2=\rho_k=0$ (all autocorrelations are zero). vs. $H_a$: at least one $\rho_k \neq 0$ (at least one autocorrealtion is not zero)

```{r}
acf(rstandard(dlm.ts6))
```

The residuals appear stationary but not white noise.

#### Residual Normality

The Shapiro-Wilk Test is performed to verify residual normality:

```{r}
hist(rstandard(dlm.ts6))
shapiro.test(rstandard(dlm.ts6))
```

As can be seen, and is confirmed by the Shapiro-Wilk test, the residuals of the model are not normally distributed.

#### Constant Variance

White's test for Heteroscedasticity is performed to confirm constant variance:

```{r}
white_lm(dlm.ts6, interactions = TRUE)
```

$H_0$: There is no Heteroscedasticity (constant variance) vs. $H_a$: There is Heteroscedasticity (no constant variance)

The resulting p-value is much greater than 0.05, therefore $H_0$ is accepted, and one can conclude that there is constant variance within the series.

### Model Summary

The harmonic deterministic model with 15 harmonics, resolves the issue of residual heteroskedasticity, but still fails the normality and autocorrelation assumptions.

# Stochastic Model

First, before fitting a stochastic model investigation must result in a stationary ACF - based on recusive tranformation to induce stationarity.

```{r}
acf(df.ts.modelset$IIPI)
```

As can be seen the raw dataset is not stationary.

Taking the first difference:

```{r}
acf(diff(df.ts.modelset$IIPI))
```

results in stationary with periodic exponential decay.

Having acheived stationarity at the first difference, the PACF is reviewed:

```{r}
pacf(diff(df.ts.modelset$IIPI))
```

This also appears to have periodic decay.

Starting with investigating an ARIMA(1,1,1) - given that ACF and PACF show periodicity and stationarity was acheived at the frist difference.



## ... Model

### Model Fit Plot

### Residual Assumptions

#### Residual Autocorrelation

#### Residual Normality

#### Constant Variance

### Model Summary

# XXXXXX Model [SECTION]

## ... Model

### Model Fit Plot

### Residual Assumptions

#### Residual Autocorrelation

#### Residual Normality

#### Constant Variance

### Model Summary





# Model Comparisions

### In-sample and Predictive Comparisons

```{r echo=FALSE, warings=FALSE}

mape <- function(d,p){
  mean(abs(d-p)/d)
}

comparetable <- tibble(
  Model = c(
    'Linear Model, variable reduced',
    'Deterministic Model, Monthly Indicator with Trend',
    'Deterministic Model, Polynomial k=5',
    'Deterministic Model, 100 Harmonics',
    'Deterministic Model, 85 Harmonics',
    'Deterministic Model, 15 Harmonics',
    'Deterministic Model, 65 Harmonics'), 
  `Adjusted R-squared` = c(summary(mlm.ts2)$adj.r.squared,
                           summary(dlm.ts1)$adj.r.squared,
                           summary(dlm.ts2)$adj.r.squared,
                           summary(dlm.ts3)$adj.r.squared,
                           summary(dlm.ts4)$adj.r.squared,
                           summary(dlm.ts6)$adj.r.squared,
                           summary(dlm.ts7)$adj.r.squared),
    MAPE = c(
    mape(d = df.ts.modelset.test$IIPI, 
         p= predict(mlm.ts2, df.ts.modelset.test %>% select(-c(IIPI,Date_form,GDP,IndPro)), interval="prediction")),
    mape(d = df.ts.modelset.deterministic.test$IIPI, 
         p= predict(dlm.ts1, df.ts.modelset.deterministic.test, interval="prediction")),
    mape(d = df.ts.modelset.deterministic.test$IIPI, 
         p= predict(dlm.ts2, df.ts.modelset.deterministic.test, interval="prediction")),
    mape(d = df.ts.modelset.harmonic.test$IIPI, 
         p= predict(dlm.ts3, df.ts.modelset.harmonic.test, interval="prediction")),
    mape(d = df.ts.modelset.harmonic.test$IIPI, 
         p= predict(dlm.ts4, df.ts.modelset.harmonic.test, interval="prediction")),
    mape(d = df.ts.modelset.harmonic.test1$IIPI, 
         p= predict(dlm.ts6, df.ts.modelset.harmonic.test1, interval="prediction")),
    mape(d = df.ts.modelset.harmonic.test2$IIPI, 
         p= predict(dlm.ts7, df.ts.modelset.harmonic.test2, interval="prediction"))
    )
)
comparetable
```





























