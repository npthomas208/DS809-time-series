---
title: "DS809 - Industrial Inputs Price Index"
author: "Nate Thomas"
date: "`r Sys.Date()`"
output: word_document
---

```{r echo = FALSE, message=FALSE, results='hide'}
load('.RData')
library(dplyr)
library(car)
library(skedastic)
library(kableExtra)
library(TSA)
library(tseries)
library(ggplot2)
library(fGarch)
```

# Introduction and Overview

This project endeavors to provide example implementation of the various methods outlined in DS809 Time Series to model the commodity Industrial Inputs Price Index from 1980 through 2016. The Industrial Input Price Index, hereafter IIPI, is provided by the International Monetary Fund (IMF) in the IMF Primary Commodity Prices Report (*IMF Primary Commodity Prices*, 2021). As a break-out index it does not include energy commodities (Coal, Natural Gas, Spot Crude, Propane) but does include Agricultural raw materials (Cotton, Hides, Rubber, Timber, Wool), metals (Aluminum, Cobalt, Copper, Iron Ore, Lead, Molybdenum, Nickel, Tin, Uranium, and Zinc), and precious metals (Gold, Palladium, Platinum, Silver). The index use are weighted average prices as representative of the global market, corrected for volume, as determined by the largest import markets for each commodity constituent. The purpose of the index is to provide a relative value indicator (here indexed to 2005 = 100) for assessment of the overall commodities market in question.

The first task performed for this project was to create the time series dataset using *Predictive Dynamics in Commodity Prices* (Gargano & Timmerman, 2012) as a loose framework for select variables therein. These  were collected and organized for subsequent analysis.  The following is the list of variables collected and their sources:

A. IIPI (Aliyev, 2020), Dependent Variable and Date index. tags: **IIPI, Date_form**. This dataset defined the time frame of interest (1980-02-01 to 2016-02-01). All other datasets were selected contingent upon their inclusion of this timeframe.

```{r echo = FALSE}
p1
```
*Plot of IIPI as a function of time*

B. 13 Week Treasury Bill (*US Trade - Statistics & Facts*, 2021), Independent Variable. tag: **TBillClose**.

C. US Real GDP (*Real Gross Domestic Product (GDPC1)* \| *FRED* \| *St. Louis Fed*, n.d.). tag: **GDP**

D. US Inflation (*US Inflation Rate by Month*, n.d.). tag: **InfRate**

E. Industrial Production (*Industrial Production: Total Index (INDPRO)*\| *FRED* \| *St. Louis Fed*, n.d.). tag: **IndPro**

F. Money Supply M2, (*BOARD OF GOVERNORS of the FEDERAL RESERVE SYSTEM*, 2021). tag: **M2**

G. US Unemployment, (*Employment and Unemployment*). tag: **UnempRate**

H. Recession, (*Dates of U.S. Recessions as Inferred by GDP-based Recession Indicator*). tag : **Recession**

The index is indicative of global market but this project uses United States specific independent variable values as analog facsimiles for global extrapolatoin. This is a recognized short-hand performed here to allow for timely and comprehensive analysis. United States' trade accounts for the largest percentage (13.5%) of total import trade worldwide (*US Trade - Statistics & Facts*, 2021), and therefore while this is not perfect it is used for the purposes of this project.

The first rows of the dataset are displayed after collection, cleaning, and being joined according to date:

```{r echo = FALSE, size = "tiny"}
head(df.ts.modelset)
```

## Visual Inspection of the Data

The data was decomposed using native R *stats* package to visually assess the potential time series affects of IIPI.

```{r echo = FALSE}
plot(decompose(df.ts))
```

*Time Series Decomposition*

## Project Goal

The goal of any time series analysis is to achieve white noise residuals by modeling the dataset.  This implies that a model has capture all non-stochastic movement of the series with respect to the independent variables, or inherent autocorrelation features within the dataset.

## Excluding White Noise Criteria

The first task presented in this project is to exclude IIPI as a purely white noise process.  A white noise process is defined by the following criteria

1) Constant Mean: $E(IIPI_t) = \mu$ for all $t$

2) Constant Variance: $Var(Y_t) = \sigma^2$ for all $t$

3) Autocorrelation is not found at any lags: $\rho_k = 0$ for all $k\geq1$

To show then that a series is not a WNP one must simply prove at least one of these constraints to be false.

### IIPI White Noise Rejection

The Box-Pierce Test is used to confirm the existence of autocorrelation up to a defined maximum lag value (here 36 months):

```{r echo = FALSE}
Box.test(df$IIPI,lag=36)
```


$H_0$: All autocorrelctions are zero (0), $H_a$: At least one (1) autocorrelation is not zero (0) 

The test indicates one can safely reject the null hypothesis ($H_0$) - such that at least one (1) autocorrleation is not zero.

This is also seen through visual inspection of the ACF chart:

```{r echo = FALSE}
acf(df$IIPI, lag.max=36)
```
Based on the autocorrelation chart displayed one can be very confident that there exists autocorrelation to a great degree, across the three years of lag values shown.  The series is non-stationary as indicated by the slow decay in autocorrelation.

Based on this, the series is not a white noise process.

### IIPI First Difference White Noise Process Rejection

Again, the Box-Pierce Q-statistic Test is used to confirm the existence of autocorrelation up to a defined maximum lag value (here 36 months) for also the first difference:

```{r echo = FALSE}
Box.test(df$IIPI_dif,lag=36)
```

$H_0$: All autocorrelctions are zero (0), $H_a$: At least one (1) autocorrelation is not zero (0) 

The test indicates one can safely reject the null hypothesis (H0) - such that at least one (1) autocorrleation is not zero in the first difference series.

This is also seen through visual inspection of the ACF chart:

```{r echo = FALSE}
acf(df$IIPI_dif, lag.max=36)
```

The first difference of the data is stationary as can be seen in the fast decay in autocorrelation, however, it is not white noise due to a variety of significant autocorrelations.

# Regression Model

A multiple linear regression is estimated:

$IIPI(`r names(mlm.ts$coefficients)[2:length(names(mlm.ts$coefficients))] %>% paste(sep = " ")`) = \hat{\beta}_0 + \hat{\beta}_1 TBillCLose + \hat{\beta}_2 GDP  + \hat{\beta}_3 InfRate + \hat{\beta}_4 IndPro + \hat{\beta}_5 M2 + \hat{\beta}_6 UnempRate + \hat{\beta}_7 Recession + \epsilon_t$

Model reduction is perfomed iteratively, and the VIFs of each iterations is shown below. Ten (10) is the typically accepted subjective value at which a variance inflation factor is considered high:

```{r echo = FALSE}
vif(mlm.ts)
```


```{r echo = FALSE}
vif(mlm.ts1)
```

```{r echo = FALSE}
vif(mlm.ts2)
```

Due to the multicollinearities this model is reduced to the following after inspection of the VIFs model:

$IIPI(`r names(mlm.ts2$coefficients)[2:length(names(mlm.ts2$coefficients))] %>% paste(sep = " ")`) = \hat{\beta}_0 + \hat{\beta}_1 TBillCLose + \hat{\beta}_3 InfRate + \hat{\beta}_5 M2 + \hat{\beta}_6 UnempRate + \hat{\beta}_7 Recession + \epsilon_t$

### Model Fit Plot

```{r echo = FALSE}
#Plotting the confidence and prediction intervals
predint =  exp(predict(mlm.ts3,df.ts.modelset,interval="prediction"))
confint =  exp(predict(mlm.ts3,df.ts.modelset,interval="confidence"))
predlower = predint[,2]
predupper = predint[,3]
conflower = confint[,2]
confupper = confint[,3]

plot(df.ts.modelset$IIPI, col="lightgray", xlab = "Time", ylab = "IIPI")
lines(predint[,1],col="red")
abline(v=424,col="blue", lty=5)
lines(predlower,col="orange")
lines(predupper,col="orange")
lines(conflower,col="blue")
lines(confupper,col="blue")
```

*Regression Model Fit*

### Residual Assumptions

Before proceeding with model interpretation the regression residual assumptions must be verified:

1. $E(\epsilon_t|X_{1t},...,X_{kt}) = 0$ (zero mean assumption)

2. $Var(\epsilon_t|X_{1t},...,X_{kt}) = Var(\epsilon_t) = \sigma^2$ for all $t$ (constant variance assumption)

3. $(\epsilon_t|X_{1t},...,X_{kt}) ~N(0,\sigma^2)$ for all $t$ (normality assumption)

4. $Cov((\epsilon_t,\epsilon_h|X_{1t},...,X_{kt}) = 0$ for $t \neq h$ (error terms are not autocorrelated along the time dimension)

#### Residual Normality

The Shapiro-Wilk Test is performed to verify residual normality:

```{r echo = FALSE}
shapiro.test(rstandard(mlm.ts2))
```

$H_0$: Series are normally distributed. vs. $H_a$: Series are not normally distributed

The resulting p-value is less than 0.05, therefore $H_0$ is rejected, and one must conclude that the series are not normally distributed.

The residuals are therefore not white noise. There other assumption verfications are performed to ensure fully rigorous analysis, however are technically unnecessary given this result. 

#### Residual Autocorrelation

The Box-Pierce test is performed to identify if the residuals are autocorrelated: 

```{r echo = FALSE}
Box.test(rstandard(mlm.ts2), lag=36)

```

$H_0$: $\rho_1=\rho_2=\rho_k=0$ (all autocorrelations are zero). vs. $H_a$: at least one $\rho_k \neq 0$ (at least one autocorrealtion is not zero)

The residuals are highly autocorrelated and non-stationary.  This model can be improved greatly, as there still appears to be time series information not captured in this model.

The resulting p-value is less than 0.05, therefore $H_0$ is rejected, and one must conclude that at least one lag value returns a statistically significant autocorrelation, as seen below:

```{r echo = FALSE}
acf(rstandard(mlm.ts2))
```

#### Constant Variance

White's test for Heteroscedasticity is performed to confirm constant variance:

```{r echo = FALSE}
white_lm(mlm.ts2, interactions = TRUE)
```

$H_0$: There is no Heteroscedasticity (constant variance) vs. $H_a$: There is Heteroscedasticity (no constant variance)

The resulting p-value is less than 0.05, therefore $H_0$ is rejected, and one must conclude that there exists non-constant variance within the residuals.

### Regression Residual Summary

All the residual assumptions failed for this initial regression.  The residuals are not white noise.  Subsequent methods and analysis are required to evaluate the complexities presented by this dataset. 

### Model Summary

```{r echo=FALSE}
summary(mlm.ts2)
```

If the residual assumptions were found to be true one could interpret the coefficients as follows. All p-values indicate significance, except the intercept and TBillClose (though the residual distribution assumptions fail making interpretation dubious).

- The expected value of IIPI at time zero (Feb, 1980) is `r mlm.ts2$coefficients[1]`.   

- The expected change in the value of IIPI, ceteris paribus, for each unit change in TBillClose is `r mlm.ts2$coefficients[2]`.

- The expected change in the value of IIPI, ceteris paribus, for each unit change in InfRate is `r mlm.ts2$coefficients[3]`.

- The expected change in the value of IIPI, ceteris paribus, for each unit change in M2 is `r mlm.ts2$coefficients[4]`.

- The expected change in the value of IIPI, ceteris paribus, for each unit change in UnempRate is `r mlm.ts2$coefficients[5]`.

- The expected change in the value of IIPI, ceteris paribus, for each unit change in Recession is `r mlm.ts2$coefficients[6]`.

# Deterministic Models

## Indicator Variable Regression Model with Trend

A month indicator variable linear regression with is estimated, correcting for linear time trend:

$IIPI(`r names(dlm.ts1$coefficients)[2:4] %>% paste(sep = " ")`,...,`r names(dlm.ts1$coefficients)[13]`) = \hat{\beta}_0 + \hat{\beta}_1 t + \hat{\beta}_{1+i}\sum_{i=1}^{12} Month_i +  \epsilon_t$

### Model Fit Plot

```{r echo = FALSE}

predint =  predict(dlm.ts1,df.ts.modelset.deterministic,interval="prediction")
confint =  predict(dlm.ts1,df.ts.modelset.deterministic,interval="confidence")
predlower = predint[,2]
predupper = predint[,3]
conflower = confint[,2]
confupper = confint[,3]

plot(df.ts.modelset.deterministic$IIPI, col="lightgray", xlab = "Time", ylab = "IIPI")
lines(predint[,1],col="red")
abline(v=424,col="blue", lty=5)
lines(predlower,col="orange")
lines(predupper,col="orange")
lines(conflower,col="blue")
lines(confupper,col="blue")
```

*Indicator Variable Regression *

### Residual Autocorrelation

The Box-Pierce test is performed to identify if the residuals are autocorrelated (see hypothesis definition above): 

```{r echo = FALSE}
Box.test(rstandard(dlm.ts1))
```

Again, this residuals are highly autoccorrelated and non-stationary. 

```{r echo = FALSE}
acf(rstandard(dlm.ts1))
```


### Model Summary

```{r echo = FALSE}
summary(dlm.ts1)
```

None of the month indicator variables are significant.

## Polynomial Model

A polynomial linear regression is estimated:

$$IIPI(t) = \hat{\beta}_0 + \hat{\beta}_1 t + \hat{\beta}_{2}t^2 + ...+ \hat{\beta}_{k}t^k + \epsilon_t$$

$k$ was found by assessment of the adjusted R-squared values and AIC values:

```{r echo=FALSE}
plot(k_list$aic_list, xlab="k",ylab="AIC")
```

*Polynomial k Search - AIC*

```{r echo=FALSE}
plot(k_list$r2adjust,xlab="k",ylab="Adj. R-squared")
```

*Polynomial k Search - Adj. R-squared*

The minimum AIC is found to be at k=14.

### Model Fit Plot

```{r echo = FALSE}
#Plotting the confidence and prediction intervals
predint =  predict(dlm.ts2,df.ts.modelset.deterministic,interval="prediction")
confint =  predict(dlm.ts2,df.ts.modelset.deterministic,interval="confidence")
predlower = predint[,2]
predupper = predint[,3]
conflower = confint[,2]
confupper = confint[,3]

plot(df.ts.modelset.deterministic$IIPI, col="lightgray", ylab="IIPI",xlab="Time", ylim = c(20,200))
lines(predint[,1],col="red")
abline(v=424,col="blue", lty=5)
lines(predlower,col="orange")
lines(predupper,col="orange")
lines(conflower,col="blue")
lines(confupper,col="blue")
```

*Polynomial Fit, k=14*

The model fits the data well in-sample, but fails within the predicution region.

### Residual Assumptions

#### Residual Autocorrelation

The Box-Pierce test is performed to identify if the residuals are autocorrelated (hypothesis outlined above): 

```{r echo = FALSE}
Box.test(rstandard(dlm.ts2))
```

$H_0$ is rejected.  Autocorrelation is present in the series.

```{r echo = FALSE}
acf(rstandard(dlm.ts2))
```

Again, the residuals are highly autoccorrelated and non-stationary.  More work is required to improve the model. There is more inherent information in the time-series component that is not captured by this model.

### Model Summary

```{r echo = FALSE}
summary(dlm.ts2)
```

This model fits the data well insample but fails in the prediction region.  While many of the coefficients in the model would be significant, as the residuals are not whtie noise, and therefore are uninterpretable.

## Harmonic Model

A de-trended harmonic model is estimated:

$IIPI(t) = \hat{\beta}_0 + \hat{\beta}_1 t + \sum{\hat{\alpha_i}sin(\frac{2\pi i}{s}t)} + \sum{\hat{\beta}_icos(\frac{2\pi i}{s}t)} + \epsilon_t$

### Periodogram

To determine the harmonics, a periodogram is reviewed:

```{r echo = FALSE}
periodogram(detrend$residuals)
```

*Periodogram*

Unfortunately, the periodogram has such high density at lower harmonics the number of peaks is not readily countable. 

Therefore, a function is used to create the model harmionic values using the top 100 harmonics identified in the Periodogram.  While this is expected to be an over estimate, the model harmonic terms are subsequently removed by evaluating that the sine and cosine pair are both not significant in the model: 

```{r eval = FALSE}
listgen <- function(t,j,h,s){
  sin_df <- tibble(rep(NA,length(t)))
  cos_df <- tibble(rep(NA,length(t)))
  for (i in 1:j){
    sinx = sin(2*pi*t*h[i]/length(s))
    cosx = cos(2*pi*t*h[i]/length(s))
    sin_df <- cbind(sin_df,sinx)
    cos_df <- cbind(cos_df,cosx)
  }
  colnames(sin_df) = c('temp',paste('sin',1:j,sep=""))
  colnames(cos_df) = c('temp',paste('cos',1:j,sep=""))
  sin_df <- sin_df %>% select(-temp)
  cos_df <- cos_df %>% select(-temp)
  df <- cbind(sin_df,cos_df)
  return(df)
}
trigf <- listgen(df.ts.modelset.harmonic$index,length(harmonics),harmonics,df.ts.modelset.harmonic$IIPI)
```

A linear model is estimated with all 200 harmonic terms (one sine and one cosine per harmonic), and those harmonics which have an insignificant (p is greater than 0.05) coefficient terms are displayed:

```{r echo = FALSE}
which(dlm.ts3.summary$coefficients[,'Pr(>|t|)']>0.05) %>% names()
```

Then those terms which have the same harmonic numeric indicator are removed - ensuring that the full harmonic (both terms) are not significant:

```{r echo=TRUE}
termsforremoval
```

The names of the terms are saved to a vector and subsequently removed from the model:

```{r echo=FALSE}
dlm.ts4.summary
```

Thee model contains 85 harmonics, all sine cosine pairs are significant (due in many cases to the other being significant).

### Model Fit Plot

This initial model with 85 harmonics significantly overfits the data.

```{r echo = FALSE}
#Plotting the confidence and prediction intervals
predint =  predict(dlm.ts4,df.ts.modelset.harmonic,interval="prediction")
confint =  predict(dlm.ts4,df.ts.modelset.harmonic,interval="confidence")
predlower = predint[,2]
predupper = predint[,3]
conflower = confint[,2]
confupper = confint[,3]

plot(df.ts.modelset.harmonic$IIPI, col="lightgray", xlab = "Time", ylab = "IIPI")
lines(predint[,1],col="red")
abline(v=424,col="blue", lty=5)
lines(predlower,col="orange")
lines(predupper,col="orange")
lines(conflower,col="blue")
lines(confupper,col="blue")
```

*Harmonic Model with 85 harmonics*

The model with 85 terms fits the in-sample data nearly perfectly. However, due to failure in the prediction region the harmonic term reduction is investigated to assess if the predictive utility can be improved.

The three in-sample metrics are displayed as a function of increasing harmonics:

```{r echo = FALSE}
plot(dlm.ts.r2tlb$r2adj, xlab = "Harmonics",ylab = "Adj. R-squared")
plot(dlm.ts.r2tlb$bic, xlab = "Harmonics",ylab = "BIC")
plot(dlm.ts.r2tlb$aic, xlab = "Harmonics",ylab = "AIC")
```

*Harmonics vs. Fit Metrics*

AIC does not minimize in this region and adjusted R-squared continues to increase. BIC has a local minimum at 65 harmonics.

```{r echo = FALSE}
#Plotting the confidence and prediction intervals
predint =  predict(dlm.ts7,df.ts.modelset.harmonic2,interval="prediction")
confint =  predict(dlm.ts7,df.ts.modelset.harmonic2,interval="confidence")
predlower = predint[,2]
predupper = predint[,3]
conflower = confint[,2]
confupper = confint[,3]

plot(df.ts.modelset.harmonic2$IIPI, col="black", xlab = "Time", ylab = "IIPI")
lines(predint[,1],col="red")
abline(v=424,col="blue", lty=5)
lines(predlower,col="orange")
lines(predupper,col="orange")
lines(conflower,col="blue")
lines(confupper,col="blue")
```

*Harmonic Model with 65 harmonics*

```{r echo = FALSE}
#Plotting the confidence and prediction intervals
predint =  predict(dlm.ts7,df.ts.modelset.harmonic2[400:433,],interval="prediction")
confint =  predict(dlm.ts7,df.ts.modelset.harmonic2[400:433,],interval="confidence")
predlower = predint[,2]
predupper = predint[,3]
conflower = confint[,2]
confupper = confint[,3]

plot(df.ts.modelset.harmonic2$IIPI[400:433], col="black",ylim = c(50,170), xlab = "Time", ylab = "IIPI")
lines(predint[,1],col="red")
abline(v=24,col="blue", lty=5)
lines(predlower,col="orange")
lines(predupper,col="orange")
lines(conflower,col="blue")
lines(confupper,col="blue")
```

*Harmonic Model with 65 harmonics*

However, this appears to still overfit the data, therefore instead in an effort to balance the bias-variance trade off the visual elbow method is used for adjusted R-squared metric.

This results in 15 harmonics before there is a reduced rate of increase in adjusted R-squared a linear model. Therefore this model is estimated:

```{r echo = FALSE}
dlm.ts6.summary
```

The terms are assessed for pair-wise significance. Only two trigonomtric terms (not of the same harmonic) are not significant.

```{r echo = FALSE}
which(dlm.ts6.summary$coefficients[,'Pr(>|t|)']>0.05) %>% names()
```
Therefore, this model is used going forward for comparison.

```{r echo = FALSE}
#Plotting the confidence and prediction intervals
predint =  predict(dlm.ts6,df.ts.modelset.harmonic1,interval="prediction")
confint =  predict(dlm.ts6,df.ts.modelset.harmonic1,interval="confidence")
predlower = predint[,2]
predupper = predint[,3]
conflower = confint[,2]
confupper = confint[,3]

plot(df.ts.modelset.harmonic1$IIPI, col="lightgray", xlab = "Time", ylab = "IIPI")
lines(predint[,1],col="red")
abline(v=424,col="blue", lty=5)
lines(predlower,col="orange")
lines(predupper,col="orange")
lines(conflower,col="blue")
lines(confupper,col="blue")
```

*Harmonic Model with 15 Harmonics*

```{r echo = FALSE}
#Plotting the confidence and prediction intervals
predint =  predict(dlm.ts6,df.ts.modelset.harmonic1[400:433,],interval="prediction")
confint =  predict(dlm.ts6,df.ts.modelset.harmonic1[400:433,],interval="confidence")
predlower = predint[,2]
predupper = predint[,3]
conflower = confint[,2]
confupper = confint[,3]

plot(df.ts.modelset.harmonic1$IIPI[400:433], col="black", xlab = "Time", ylab = "IIPI")
lines(predint[,1],col="red")
abline(v=24,col="blue", lty=5)
lines(predlower,col="orange")
lines(predupper,col="orange")
lines(conflower,col="blue")
lines(confupper,col="blue")
```

*Harmonic Model with 15 Harmonics - Prediction Region*

### Residual Assumptions (model with 15 harmonics)

#### Residual Autocorrelation

The Box-Pierce test is performed to identify if residuals are autocorrelated for the model with 15 harmonics (hypothesis defined above): 

```{r echo = FALSE}
Box.test(rstandard(dlm.ts6), lag=36)
```

```{r echo = FALSE}
acf(rstandard(dlm.ts6))
```

The residuals appear stationary but not white noise, with fast exponential periodic decay.

# Stochastic Models

## ARIMA Models

First, before fitting a stochastic model the series must be stationary:

```{r echo = FALSE}
acf(df.ts.modelset$IIPI)
```

As can be seen the raw dataset is not stationary. Taking the first difference results in stationarity with exponential periodic decay:

```{r echo = FALSE}
acf(diff(df.ts.modelset$IIPI))
```

Having acheived stationarity at the first difference, the PACF is reviewed to determine the ARIMA model parameters:

```{r echo = FALSE}
pacf(diff(df.ts.modelset$IIPI))
```

In reviewing the ACF and PACF there are three proposed reasonable ARIMA models for investigation:

- ARIMA(2,1,0) due to perceived cutoff of the PACF at 2, and interpretable exponenital decay of the ACF.

- ARIMA(0,1,5) due to percevied cutoff of the ACF at 5, and interpretable exponential decay of the PACF.

- ARIMA(x,1,y) due to the perceived exponential decay of both ACF and PACF, x and y = {0,1,2}

This assessment is validated using an subset search of p and q values.  The resulting model AIC are dispalyed:

```{r echo=FALSE}
ggplot(arima_table1, aes(x = p_index, y = q_index, fill = arima_aic)) + 
  geom_tile(color = "black") + 
  scale_fill_gradientn(colors = hcl.colors(20,"RdYlGn")) + 
  coord_fixed()
```

*ARIMA model AIC Heatmap*

The expected listed models resulted in the lowest AIC values, verifying the intuition and methodological approach:

```{r echo=FALSE}
arima_table1 %>% arrange(arima_aic) %>% head(3)
```

All three are estimated and retained for comparison given their comparative AICs.

```{r echo=FALSE}
arima210
arima111
arima015
```

As can be seen above, all coefficients are signifcant for each model except $\theta_3$ and $\theta_4$ of the MA(5) model.  Therefore this one is dropped for evaluation going forward.

The ARIMA(2,1,0) and ARIMA(1,1,1) equations follow:

ARIMA(2,1,0):

$z  = IIPI_{t} - IIPI_{t-1}$

$z_{t} = \phi_{1}z_{t-1} +  \phi_{2}z_{t-2}  + \epsilon_{t}$

ARIMA(1,1,1):

$z_{t} = \phi_{1}z_{t-1} + \epsilon_{t} - \theta_{1}\epsilon_{t-1}$

### Model Fit Plots

```{r echo = FALSE}
plot(df.ts.modelset$IIPI, col="lightgray", xlab = "Time", ylab = "IIPI")
title("ARIMA(2,1,0)")
lines(pred.arima210nahead,col="red")
abline(v=424,col="blue", lty=5)
```

*ARIMA(2,1,0) Model with 20 step ahead predictions*

However when looking at sequential prediction, the model does incredibly well in the prediction region:

```{r echo = FALSE}
plot(df.ts.modelset$IIPI[424:433], col="black", xlab = "Time", ylab = "IIPI")
title("ARIMA(2,1,0) sequential")
lines(pred.arima2101step,col="red")
abline(v=424,col="blue", lty=5)
```

*ARIMA(2,1,0) Model with sequential prediction*

```{r echo = FALSE}
plot(df.ts.modelset$IIPI, col="lightgray", xlab = "Time", ylab = "IIPI")
title("ARIMA(1,1,1)")
lines(pred.arima111nahead,col="red")
abline(v=424,col="blue", lty=5)
```

*ARIMA(1,1,1) Model with 20 step ahead predictions*

```{r echo = FALSE}
plot(df.ts.modelset$IIPI[424:433], col="black", xlab = "Time", ylab = "IIPI")
title("ARIMA(1,1,1) sequential")
lines(pred.arima1111step,col="red")
abline(v=424,col="blue", lty=5)

```

*ARIMA(1,1,1) Model with sequential prediction*

Both models perform will and are retained when comparing predictions.

### Confirming White Noise

The residuals are review for white noise.

```{r echo = FALSE}
Box.test(arima210$residuals, lag=20)
```

```{r echo = FALSE}
Box.test(arima111$residuals, lag=20)
```

Both ar marginally white noise based on the above Box-Pierce tests.

## Stoachastic Residual Correction for Regression Model

Again the regression model was:

$IIPI(`r names(mlm.ts2$coefficients)[2:length(names(mlm.ts2$coefficients))] %>% paste(sep = " ")`) = \hat{\beta}_0 + \hat{\beta}_1 TBillCLose + \hat{\beta}_3 InfRate + \hat{\beta}_5 M2 + \hat{\beta}_6 UnempRate + \hat{\beta}_7 Recession + \epsilon_t$

The residuals were not stationary, therefore the first difference is taken as then confirmed for stationarity:

```{r echo = FALSE}
acf(diff(mlm.ts4$residuals))
```

Next the PACF is investigated for ARIMA parameter confirmation:

```{r echo = FALSE}
pacf(diff(mlm.ts4$residuals))
```

As both appear potentially cuff off without clear seasonal spikes a search is performed for reasonable $p$ and $q$s

```{r echo=FALSE}
ggplot(arima_table3 %>% dplyr::filter(p_index != 0), aes(x = p_index, y = q_index, fill = arima_aic)) + 
  geom_tile(color = "black") + 
  scale_fill_gradientn(colors = hcl.colors(20,"RdYlGn")) + 
  coord_fixed()
```

*ARIMA model AIC Heatmap*

As seen in the above chart, the minimum AIC occurs at p=3, q=0. Therefore an ARIMA(3,0,0) is fit to the residuals.

```{r echo=FALSE}
arima_table3 %>% arrange(arima_aic) %>% head(3)
```

### Model Fit Plot

```{r echo = FALSE}
plot(df.ts.modelset$IIPI, col="lightgray", xlab = "Time", ylab = "IIPI")
title("Regression with ARIMA(3,0,0) n ahead")
lines(pred.mlm.ts2.arima300nahead,col="red")
abline(v=424,col="blue", lty=5)
```

*Regression with ARIMA(3,0,0) n ahead*

However when looking at on-step head values, the prediciton clearly performs much better:

```{r echo = FALSE}
plot(df.ts.modelset$IIPI[424:433], col="black", xlab = "Time", ylab = "IIPI")
title("Regression with ARIMA(3,0,0) sequential")
lines(pred.mlm.ts2.arima3001step,col="red")
abline(v=424,col="blue", lty=5)

```

*Regression with ARIMA(3,0,0) sequentia - prediction region*

### Model Summary

```{r echo = FALSE}
mlm.ts2.arima300
```

All the regression coefficients become insignificant except InfRate.

### Confirming White Noise

```{r echo = FALSE}
acf(mlm.ts2.arima300$residuals)
Box.test(mlm.ts2.arima300$residuals, lag=20)
```

The residuals are just barely white noise.

## Stochastic Residual Correction for Harmonic Model

Again the harmonic model was:

$IIPI(t) = \hat{\beta}_0 + \hat{\beta}_1 t + \sum{\hat{\alpha_i}sin(\frac{2\pi i}{s}t)} + \sum{\hat{\beta}_icos(\frac{2\pi i}{s}t)} + \epsilon_t$

First, as a reminder the residuals were very clearly auto-correlated, but already stationary:

```{r echo = FALSE}
acf(dlm.ts6$residuals)
```

The PACF is investigated to determine the appropriate ARIMA model:

```{r echo = FALSE}
pacf(dlm.ts6$residuals)
```

Both the ACF and PACF do not have a clear cut of expecting an ARMA(p,q) with p and q not equal to 0.  

```{r echo=FALSE}
ggplot(arima_table2, aes(x = p_index, y = q_index, fill = arima_aic)) + 
  geom_tile(color = "black") + 
  scale_fill_gradientn(colors = hcl.colors(20,"RdYlGn")) + 
  coord_fixed()
```

*ARIMA Model AIC Heatmap*

```{r echo=FALSE}
arima_table2 %>% arrange(arima_aic) %>% head(3)
```

As seen in the above chart ARIMA(2,0,1) best fits the residuals.

### Model Fit Plot

```{r echo = FALSE}
plot(df.ts.modelset$IIPI, col="lightgray", xlab = "Time", ylab = "IIPI")
title("Regression with ARIMA(3,0,0) n ahead")
lines(pred.dlm.ts6.arima201nahead,col="red")
abline(v=424,col="blue", lty=5)
```

*Regression with ARIMA(3,0,0) n ahead*

However when looking at on-step head values, the prediciton clearly performs much better:

```{r echo = FALSE}
plot(df.ts.modelset$IIPI[424:433], col="black", xlab = "Time", ylab = "IIPI")
title("Regression with ARIMA(3,0,0) sequential")
lines(pred.dlm.ts6.arima2011step[424:433],col="red")
abline(v=424,col="blue", lty=5)
```

*Regression with ARIMA(3,0,0) sequential - prediction region*

### Model Summary

```{r echo = FALSE}
dlm.ts6.arima201
```

Only one pair of harmonic terms (sin22 and cos22) have been made insignificant by this addition.

The AIC has been clearly improved:

Harmonic Model: `r AIC(dlm.ts6)`

Harmonic Model with residual ARIMA correction: `r dlm.ts6.arima201$aic`

### Confirming White Noise

```{r echo = FALSE}
acf(dlm.ts6.arima201$residuals)
Box.test(dlm.ts6.arima201$residuals, lag=20)
```

The residuals are not white noise.

## Generalized Autoregressive conditionally Heteroscedastic Model (GARCH)

Investigating using the GARCH model for IIPI.  First, as shown before the series and its first difference are not white noise. Therefore, a GARCH model on the original series is not applicable.

## Regression Model GARCH on residuals investigation

The residuals of the regression model are autocorrelated, therefore GARCH is not applicable:

```{r}
acf(mlm.ts2$residuals)
```

## Seasonal Model GARCH on residuals investigation

The residuals of the regression model autocorrelated, therefore GARCH is not applicable:

```{r}
acf(dlm.ts1$residuals)
```

## Polynomial Model GARCH on residuals investigation

The residuals of the regression model autocorrelated, therefore GARCH is not applicable:

```{r}
acf(dlm.ts2$residuals)
```

## Harmonic Model with 15 harmonics, GARCH on residuals investigation

The residuals of the regression model autocorrelated, therefore GARCH is not applicable:

```{r}
acf(dlm.ts6$residuals)
```

## ARIMA Model Generalized Autoregressive conditionally Heteroscedastic Model (GARCH)

Using the ARIMA(1,1,1) residuals first to check for GARCH model potential:

```{r echo=FALSE}
acf(arima111$residuals)
Box.test(arima111$residuals)

acf(arima111$residuals^2)
Box.test(arima111$residuals^2)
```

As can be seen the original ARIMA(1,1,1) model residuals are white noise, but the squared residuals are not.  This is an excellent candidate for using GARCH.

```{r echo=FALSE}
pacf(arima111$residuals^2)
```

Two potential GARCH models are investigated: GARCH(2,0) and GARCH(1,1)

### Model Summary

ARIMA(1,1,1) with GARCH(2,0)

```{r echo=FALSE}
arima111.garch20
```

All model paramaters for GARCH(2,0) are significant.  However, the residuals of this model are not white noise.

```{r echo=FALSE}
acf(arima111.garch20@residuals/arima111.garch20@sigma.t, main="Residuals")
Box.test(arima111.garch20@residuals/arima111.garch20@sigma.t, lag=20)
```

The residuals of the model are not white noise.

ARIMA(1,1,1) with  GARCH(1,1):

```{r echo=FALSE}
arima111.garch11
```

All model parameters are significant, the mean was removed based on AIC evaluation.

```{r echo=FALSE}
acf(arima111.garch11@residuals/arima111.garch11@sigma.t, main="Residuals")
Box.test(arima111.garch11@residuals/arima111.garch11@sigma.t, lag=20)
```

This residuals of this model are white noise!

The following is the predicted conditional variance of the first differenced IIPI:

```{r echo=FALSE}
predict(arima111.garch11, plot = TRUE)
```

As expected the mean value tends toward an asymptotic value:

```{r}
plot(df.ts.modelset$IIPI, col="black", xlab = "Time", ylab = "IIPI")
title("ARIMA(1,1,1) with GARCH(1,1) n ahead")
lines(pred.arima111.garch11nahead,col="red")
abline(v=424,col="blue", lty=5)
```

Zoomed in on the prediction area for better visibility:

```{r}
plot(df.ts.modelset$IIPI[423:434], col="black", xlab = "Time", ylab = "IIPI")
title("ARIMA(1,1,1) with GARCH(1,1) n ahead")
lines(pred.arima111.garch11nahead[422:433],col="red")
abline(v=424,col="blue", lty=5)
```

However, when taking the predictions 1 step ahead, the model predicts unprecedentedly well (!):

```{r}
plot(df.ts.modelset$IIPI[423:434], col="black", xlab = "Time", ylab = "IIPI")
title("ARIMA(1,1,1) with GARCH(1,1) sequential (!)")
lines(pred.arima111.garch11_1step[422:433],col="red")
abline(v=424,col="blue", lty=5)
```

##

```{r echo=FALSE, warings=FALSE}

mape <- function(d,p){
  mean(abs((d-p)/d))
}

comparetable <- tibble(
  Model = c(
    'Linear Model, variable reduced',
    'Deterministic Model, Monthly Indicator with Trend',
    'Deterministic Model, Polynomial k=14',
    'Deterministic Model, 100 Harmonics',
    'Deterministic Model, 85 Harmonics',
    'Deterministic Model, 15 Harmonics',
    'Deterministic Model, 65 Harmonics',
    'ARIMA(2,1,0) n ahead',
    'ARIMA(1,1,1) n ahead',
    'ARIMA(0,1,5) n ahead',
    'ARIMA(2,1,0) seq. 1 ahead',
    'ARIMA(1,1,1) seq. 1 ahead',
    'ARIMA(0,1,5) seq. 1 ahead',
    'Linear Model, v.r., residual ARIMA, n ahead',
    'Linear Model, v.r., residual ARIMA, seq. 1 ahead',
    'Harmonic Model, residual ARIMA, n ahead',
    'Harmonic Model, residual ARIMA, seq. 1 ahead',
    'ARIMA(1,1,1) with GARCH(1,1), n ahead',
    'AIMAA(1,1,1) with GARCH(1,1) seq. 1 ahead'),
    MAPE = c(
    mape(d = df.ts.modelset.test$IIPI, 
         p = predict(mlm.ts2, df.ts.modelset.test %>% select(-c(IIPI,Date_form,GDP,IndPro)))),
    mape(d = df.ts.modelset.deterministic.test$IIPI, 
         p = predict(dlm.ts1, df.ts.modelset.deterministic.test)),
    mape(d = df.ts.modelset.deterministic.test$IIPI, 
         p = predict(dlm.ts2, df.ts.modelset.deterministic.test)),
    mape(d = df.ts.modelset.harmonic.test$IIPI, 
         p = predict(dlm.ts3, df.ts.modelset.harmonic.test)),
    mape(d = df.ts.modelset.harmonic.test$IIPI, 
         p = predict(dlm.ts4, df.ts.modelset.harmonic.test)),
    mape(d = df.ts.modelset.harmonic.test1$IIPI, 
         p = predict(dlm.ts6, df.ts.modelset.harmonic.test1)),
    mape(d = df.ts.modelset.harmonic.test2$IIPI, 
         p = predict(dlm.ts7, df.ts.modelset.harmonic.test2)),
    mape(d = df.ts.modelset.test$IIPI, 
         p = predict(arima210, n.ahead =10)$pred),
    mape(d = df.ts.modelset.test$IIPI, 
         p = predict(arima111, n.ahead =10)$pred),    
    mape(d = df.ts.modelset.test$IIPI, 
         p = predict(arima015, n.ahead =10)$pred),
    mape(d = df.ts.modelset.test$IIPI, 
         p = pred.arima2101step),
    mape(d = df.ts.modelset.test$IIPI, 
         p = pred.arima1111step),
    mape(d = df.ts.modelset.test$IIPI, 
         p = pred.arima0151step),
    mape(d = df.ts.modelset.test$IIPI,
         p = pred.mlm.ts2.arima300nahead[424:433]),
    mape(d = df.ts.modelset.test$IIPI,
         p = pred.mlm.ts2.arima3001step[424:433]),
    mape(d = df.ts.modelset.test$IIPI,
         p = pred.dlm.ts6.arima201nahead[424:433]),
    mape(d = df.ts.modelset.test$IIPI,
         p = pred.dlm.ts6.arima2011step[424:433]),
    mape(d = df.ts.modelset.test$IIPI,
         p = pred.arima111.garch11nahead[423:432]),
    mape(d = df.ts.modelset.test$IIPI,
         p = pred.arima111.garch11_1step[423:432]) 
    )
)
comparetable %>% arrange(MAPE)
```





























