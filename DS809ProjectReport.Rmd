---
title: "DS809 - Industrial Inputs Price Index Modelling"
author: "Nate Thomas"
date: "`r Sys.Date()`"
output: word_document
---

```{r echo = FALSE, message=FALSE, results='hide'}
load('.RData')
library(dplyr)
library(car)
library(skedastic)
library(kableExtra)
library(TSA)
library(tseries)
library(ggplot2)
library(fGarch)
library(rugarch)
```

# Introduction and Overview

This project endeavors to provide example implementation of the various methods outlined in DS809 Time Series.  The commodity index Industrial Inputs Price Index is modelled from 1980 through 2016. The Industrial Input Price Index, hereafter IIPI, is provided by the International Monetary Fund (IMF) in the IMF Primary Commodity Prices Report (*IMF Primary Commodity Prices*, 2021). As a break-out index, it does not include energy commodities (Coal, Natural Gas, Spot Crude, Propane) but does include agricultural raw materials (Cotton, Hides, Rubber, Timber, Wool), metals (Aluminum, Cobalt, Copper, Iron Ore, Lead, Molybdenum, Nickel, Tin, Uranium, and Zinc), and precious metals (Gold, Palladium, Platinum, Silver). The index uses weighted average prices as representative of the global market, corrected for volume, as determined by the largest import markets for each commodity constituent. The purpose of the index is to provide a relative value indicator (here indexed to 2005 = 100) for assessment of the overall commodities' market in question.

The first task performed for this project was to create the time series dataset using *Predictive Dynamics in Commodity Prices* (Gargano & Timmerman, 2012) as a loose framework for select variables therein. These  were collected and organized for subsequent analysis.  The following is the list of variables collected and their sources:

A. IIPI (Aliyev, 2020), Dependent Variable and Date index. tags: **IIPI, Date_form**. This dataset defined the time frame of interest (1980-02-01 to 2016-02-01). All other datasets were selected contingent upon their inclusion of this timeframe.

```{r echo = FALSE}
p1
```

*Plot of IIPI as a function of time*

B. 13 Week Treasury Bill (*US Trade - Statistics & Facts*, 2021), Independent Variable. tag: **TBillClose**.

C. US Real GDP (*Real Gross Domestic Product (GDPC1)* \| *FRED* \| *St. Louis Fed*, n.d.). tag: **GDP**

D. US Inflation (*US Inflation Rate by Month*, n.d.). tag: **InfRate**

E. Industrial Production (*Industrial Production: Total Index (INDPRO)*\| *FRED* \| *St. Louis Fed*, n.d.). tag: **IndPro**

F. Money Supply M2, (*BOARD OF GOVERNORS of the FEDERAL RESERVE SYSTEM*, 2021). tag: **M2**

G. US Unemployment, (*Employment and Unemployment*). tag: **UnempRate**

H. Recession, (*Dates of U.S. Recessions as Inferred by GDP-based Recession Indicator*). tag : **Recession**

IIPI is indicative of global market but this project uses United States specific independent variable values as analog facsimiles for global extrapolation. This is a recognized short-hand performed here to allow for timely and comprehensive analysis. United States' trade accounts for the largest percentage (13.5%) of total import trade worldwide (*US Trade - Statistics & Facts*, 2021), and therefore while this is not perfect US specific data is used for the purposes of this project.

## Visual Inspection of the Data

The data was decomposed using native R *stats* package to visually assess the potential time series decomposition of IIPI.

```{r echo = FALSE}
plot(decompose(df.ts))
```

*Time Series Decomposition*

The scale of the seasonal component is misleading, and is found to be insignificant in subsequent analysis to follow.  There is also very clearly non-constant variance in the series as seen in the randomness plot.

## Project Goal

The goal of any time series analysis is to achieve white noise residuals.  By achieving white noise residuals one can conclude that a model has captured the non-stochastic behavior of the modeled series.

## Excluding White Noise Criteria

The first task presented in this project is to exclude IIPI as a purely white noise process.  If it were white noise then modelling it would be trivial and effectively complete.  A white noise process is defined by the following criteria:

1) Constant Mean: $E(IIPI_t) = \mu$ for all $t$

2) Constant Variance: $Var(Y_t) = \sigma^2$ for all $t$

3) Autocorrelation is not found at any lags: $\rho_k = 0$ for all $k\geq1$

To show then that a series is not a white noise process one must simply prove at least one of these constraints to be false.

### IIPI White Noise Rejection

The Box-Pierce Test is used to confirm the existence of autocorrelation up to a defined maximum lag value (here 36 months):

```{r echo = FALSE}
Box.test(df$IIPI,lag=36)
```


$H_0$: All autocorrelctions are zero (0), $H_a$: At least one (1) autocorrelation is not zero (0) 

The test indicates one can safely reject the null hypothesis ($H_0$) - such that at least one (1) autocorrleation is not zero.

This is also seen through visual inspection of the autocorrelation function (ACF):

```{r echo = FALSE}
acf(df$IIPI, lag.max=36)
```

The series is non-stationary as indicated by the slow decay in autocorrelation.

Based on this, the series is not a white noise process.

### IIPI First Difference White Noise Process Rejection

The first difference is also reviewed to ensure it is not white noise.

Again, the Box-Pierce Test is used to confirm the existence of autocorrelation up to a defined maximum lag value (here 36 months) for the first difference:

```{r echo = FALSE}
Box.test(df$IIPI_dif,lag=36)
```

$H_0$: All autocorrelctions are zero (0), $H_a$: At least one (1) autocorrelation is not zero (0) 

The test indicates one can safely reject the null hypothesis (H0) - such that at least one (1) autocorrleation is not zero in the first difference of the series.

This is also seen through visual inspection of the ACF chart:

```{r echo = FALSE}
acf(df$IIPI_dif, lag.max=36)
```

The first difference of IIPI is stationary as can be seen in the fast decay in autocorrelation. However, it is not white noise due to a variety of significant autocorrelations (those seen beyond the dotted blue line).

# Regression Model

A multiple linear regression is estimated:

$IIPI(`r names(mlm.ts$coefficients)[2:length(names(mlm.ts$coefficients))] %>% paste(sep = " ")`) = \hat{\beta}_0 + \hat{\beta}_1 TBillCLose + \hat{\beta}_2 GDP  + \hat{\beta}_3 InfRate + \hat{\beta}_4 IndPro + \hat{\beta}_5 M2 + \hat{\beta}_6 UnempRate + \hat{\beta}_7 Recession + \epsilon_t$

Model reduction is perfomed iteratively, and the VIFs of each iterations is shown below. Ten (10) is the typically accepted subjective value at which a variance inflation factor is considered high.  Only one independent variable is removed at each iteration.  First GDP then IndPro are removed:

Iteration 1:

```{r echo = FALSE}
kable(vif(mlm.ts),"simple")
```

Iteration 2:

```{r echo = FALSE}
kable(vif(mlm.ts1),"simple")
```

Iteration 3. The VIFs of the variable reduced model are shown below. This model is retained for subsequent analysis:

```{r echo = FALSE}
kable(vif(mlm.ts2),"simple")
```

Due to the multicollinearities the regression model has been reduced to the following:

$IIPI(`r names(mlm.ts2$coefficients)[2:length(names(mlm.ts2$coefficients))] %>% paste(sep = " ")`) = \hat{\beta}_0 + \hat{\beta}_1 TBillCLose + \hat{\beta}_3 InfRate + \hat{\beta}_5 M2 + \hat{\beta}_6 UnempRate + \hat{\beta}_7 Recession + \epsilon_t$

### Model Fit Plot

The model is fit to the first 423 observations, then ten test predictions are estimated (after the blue dotted line):

```{r echo = FALSE}
#Plotting the confidence and prediction intervals
predint =  exp(predict(mlm.ts3,df.ts.modelset,interval="prediction"))
confint =  exp(predict(mlm.ts3,df.ts.modelset,interval="confidence"))
predlower = predint[,2]
predupper = predint[,3]
conflower = confint[,2]
confupper = confint[,3]

plot(df.ts.modelset$IIPI, col="lightgray", xlab = "Time", ylab = "IIPI")
lines(predint[,1],col="red")
abline(v=424,col="blue", lty=5)
lines(predlower,col="orange")
lines(predupper,col="orange")
lines(conflower,col="blue")
lines(confupper,col="blue")
```

*Regression Model Fit*

### Residual Assumptions

Before proceeding with model interpretation the regression residual assumptions must be verified:

1. $E(\epsilon_t|X_{1t},...,X_{kt}) = 0$ (zero mean assumption)

2. $Var(\epsilon_t|X_{1t},...,X_{kt}) = Var(\epsilon_t) = \sigma^2$ for all $t$ (constant variance assumption)

3. $(\epsilon_t|X_{1t},...,X_{kt}) ~N(0,\sigma^2)$ for all $t$ (normality assumption)

4. $Cov((\epsilon_t,\epsilon_h|X_{1t},...,X_{kt}) = 0$ for $t \neq h$ (error terms are not autocorrelated along the time dimension)

#### Residual Normality

The Shapiro-Wilk Test is performed to verify residual normality:

```{r echo = FALSE}
shapiro.test(rstandard(mlm.ts2))
```

$H_0$: Series are normally distributed. vs. $H_a$: Series are not normally distributed

The resulting p-value is less than 0.05, therefore $H_0$ is rejected, and one must conclude that the series are not normally distributed.

The residuals are therefore not white noise. The other assumptions are verified to ensure fully rigorous analysis, however are technically unnecessary given this result. 

#### Residual Autocorrelation

The Box-Pierce test is performed to identify if the residuals are autocorrelated: 

```{r echo = FALSE}
Box.test(rstandard(mlm.ts2), lag=36)

```

$H_0$: $\rho_1=\rho_2=\rho_k=0$ (all autocorrelations are zero). vs. $H_a$: at least one $\rho_k \neq 0$ (at least one autocorrealtion is not zero)

The residuals are highly autocorrelated and non-stationary.  This model can be improved greatly, as there still appears to be time series information not captured in this model.

The resulting p-value is less than 0.05, therefore $H_0$ is rejected, and one must conclude that at least one lag value returns a statistically significant autocorrelation, as seen below:

```{r echo = FALSE}
acf(rstandard(mlm.ts2))
```

#### Constant Variance

White's test for heteroscedasticity is performed to confirm constant variance:

```{r echo = FALSE}
white_lm(mlm.ts2, interactions = TRUE)
```

$H_0$: There is no Heteroscedasticity (constant variance) vs. $H_a$: There is Heteroscedasticity (no constant variance)

The resulting p-value is less than 0.05, therefore $H_0$ is rejected, and one must conclude that there exists non-constant variance within the residuals.

### Regression Residual Summary

All the residual assumptions failed for this initial regression.  The residuals are not white noise.  Subsequent methods and analysis will be required to extract inherent non-stochastic information presented in the IIPI series. 

### Model Summary

```{r echo=FALSE}
summary(mlm.ts2)
```

If the residual assumptions were found to be true one could interpret the coefficients displayed above. All p-values indicate significance, except the intercept and TBillClose (though the residual distribution assumptions fail making interpretation dubious).

- The expected value of IIPI at time zero (Feb, 1980) is `r mlm.ts2$coefficients[1]`.   

- The expected change in the value of IIPI, ceteris paribus, for each unit change in TBillClose is `r mlm.ts2$coefficients[2]`.

- The expected change in the value of IIPI, ceteris paribus, for each unit change in InfRate is `r mlm.ts2$coefficients[3]`.

- The expected change in the value of IIPI, ceteris paribus, for each unit change in M2 is `r mlm.ts2$coefficients[4]`.

- The expected change in the value of IIPI, ceteris paribus, for each unit change in UnempRate is `r mlm.ts2$coefficients[5]`.

- The expected change in the value of IIPI, ceteris paribus, for each unit change in Recession is `r mlm.ts2$coefficients[6]`.

# Deterministic Models

## Indicator Variable Regression Model with Trend

A month indicator variable linear regression is estimated, correcting for linear time trend:

$IIPI(`r names(dlm.ts1$coefficients)[2:4] %>% paste(sep = " ")`,...,`r names(dlm.ts1$coefficients)[13]`) = \hat{\beta}_0 + \hat{\beta}_1 t + \hat{\beta}_{1+i}\sum_{i=1}^{12} Month_i +  \epsilon_t$

### Model Fit Plot

```{r echo = FALSE}

predint =  predict(dlm.ts1,df.ts.modelset.deterministic,interval="prediction")
confint =  predict(dlm.ts1,df.ts.modelset.deterministic,interval="confidence")
predlower = predint[,2]
predupper = predint[,3]
conflower = confint[,2]
confupper = confint[,3]

plot(df.ts.modelset.deterministic$IIPI, col="lightgray", xlab = "Time", ylab = "IIPI")
lines(predint[,1],col="red")
abline(v=424,col="blue", lty=5)
lines(predlower,col="orange")
lines(predupper,col="orange")
lines(conflower,col="blue")
lines(confupper,col="blue")
```

*Indicator Variable Regression *

### Residual Autocorrelation

The Box-Pierce test is performed to identify if the residuals are autocorrelated (see hypothesis definition above): 

```{r echo = FALSE}
Box.test(rstandard(dlm.ts1))
```

Again, the residuals are highly autoccorrelated and non-stationary. 

```{r echo = FALSE}
acf(rstandard(dlm.ts1))
```

### Model Summary

```{r echo = FALSE}
summary(dlm.ts1)
```

None of the month indicator variables are significant.

## Polynomial Model

A polynomial linear regression is estimated:

$$IIPI(t) = \hat{\beta}_0 + \hat{\beta}_1 t + \hat{\beta}_{2}t^2 + ...+ \hat{\beta}_{k}t^k + \epsilon_t$$

$k$ was found by assessment of the adjusted R-squared values and AIC values of prospective models:

```{r echo=FALSE}
plot(k_list$aic_list, xlab="k",ylab="AIC")
```

*Polynomial k Search - AIC*

```{r echo=FALSE}
plot(k_list$r2adjust,xlab="k",ylab="Adj. R-squared")
```

*Polynomial k Search - Adj. R-squared*

The minimum AIC is found to be at $k=14$.

### Model Fit Plot

```{r echo = FALSE}
#Plotting the confidence and prediction intervals
predint =  predict(dlm.ts2,df.ts.modelset.deterministic,interval="prediction")
confint =  predict(dlm.ts2,df.ts.modelset.deterministic,interval="confidence")
predlower = predint[,2]
predupper = predint[,3]
conflower = confint[,2]
confupper = confint[,3]

plot(df.ts.modelset.deterministic$IIPI, col="lightgray", ylab="IIPI",xlab="Time", ylim = c(20,200))
lines(predint[,1],col="red")
abline(v=424,col="blue", lty=5)
lines(predlower,col="orange")
lines(predupper,col="orange")
lines(conflower,col="blue")
lines(confupper,col="blue")
```

*Polynomial Fit, k=14*

The model fits the data well in-sample, but performes poorly within the prediction region.

### Residual Assumptions

#### Residual Autocorrelation

The Box-Pierce test is performed to identify if the residuals are autocorrelated (hypothesis outlined above): 

```{r echo = FALSE}
Box.test(rstandard(dlm.ts2))
```

$H_0$ is rejected.  Autocorrelation is present in the series.

```{r echo = FALSE}
acf(rstandard(dlm.ts2))
```

Again, the residuals are highly autoccorrelated and non-stationary.  More work is required to better capture the inherent information contained within the IIPI series.

### Model Summary

```{r echo = FALSE}
summary(dlm.ts2)
```

This model fits the in-sample data well but fails in the prediction region.  While many of the coefficients in the model would be significant, the residuals are not white noise, therefore their interpretation is not rigorously permitted, and is foregone.

## Harmonic Model

A de-trended harmonic model is estimated:

$IIPI(t) = \hat{\beta}_0 + \hat{\beta}_1 t + \sum{\hat{\alpha}_isin(\frac{2\pi i}{s}t)} + \sum{\hat{\beta}_icos(\frac{2\pi i}{s}t)} + \epsilon_t$

### Periodogram

To determine the harmonics, a periodogram is reviewed:

```{r echo = FALSE}
periodogram(detrend$residuals)
```

*Periodogram*

Unfortunately, the periodogram has such high density at lower harmonics the number of peaks is not readily countable through visual inspection. Therefore, a function is used to create the model harmionic values using the top 100 harmonics identified in the Periodogram.  While this is expected to be an over estimate, the model harmonic terms are subsequently removed by evaluating that the sine and cosine pair are both not significant in the model: 

```{r eval = FALSE}
listgen <- function(t,j,h,s){
  sin_df <- tibble(rep(NA,length(t)))
  cos_df <- tibble(rep(NA,length(t)))
  for (i in 1:j){
    sinx = sin(2*pi*t*h[i]/length(s))
    cosx = cos(2*pi*t*h[i]/length(s))
    sin_df <- cbind(sin_df,sinx)
    cos_df <- cbind(cos_df,cosx)
  }
  colnames(sin_df) = c('temp',paste('sin',1:j,sep=""))
  colnames(cos_df) = c('temp',paste('cos',1:j,sep=""))
  sin_df <- sin_df %>% select(-temp)
  cos_df <- cos_df %>% select(-temp)
  df <- cbind(sin_df,cos_df)
  return(df)
}
trigf <- listgen(df.ts.modelset.harmonic$index,length(harmonics),harmonics,df.ts.modelset.harmonic$IIPI)
```

A linear model is estimated with all 200 harmonic terms (one sine and one cosine per harmonic), and those harmonics which have an insignificant (p is greater than 0.05) coefficient terms are displayed:

```{r echo = FALSE}
which(dlm.ts3.summary$coefficients[,'Pr(>|t|)']>0.05) %>% names()
```

Then those terms which have the same harmonic numeric indicator are removed - ensuring that the full harmonic (both terms) are not significant:

```{r echo=TRUE}
termsforremoval
```

### Model Summary

The names of the terms are saved to a vector and subsequently removed from the model. The model is then estimated and dispalyed below:

```{r echo=FALSE}
dlm.ts4.summary
```

The model contains 85 harmonics, all sine cosine pairs are significant (due in many cases to the other respective term being significant).

### Model Fit Plot

This initial model with 85 harmonics significantly overfits the data.

```{r echo = FALSE}
#Plotting the confidence and prediction intervals
predint =  predict(dlm.ts4,df.ts.modelset.harmonic,interval="prediction")
confint =  predict(dlm.ts4,df.ts.modelset.harmonic,interval="confidence")
predlower = predint[,2]
predupper = predint[,3]
conflower = confint[,2]
confupper = confint[,3]

plot(df.ts.modelset.harmonic$IIPI, col="lightgray", xlab = "Time", ylab = "IIPI")
lines(predint[,1],col="red")
abline(v=424,col="blue", lty=5)
lines(predlower,col="orange")
lines(predupper,col="orange")
lines(conflower,col="blue")
lines(confupper,col="blue")
```

*Harmonic Model with 85 harmonics*

The model with 85 terms fits the in-sample data nearly perfectly (Adjusted R-squared:  0.9979). However, due to failure in the prediction region harmonic term reduction is investigated to assess if the predictive utility can be improved.

The three in-sample metrics are displayed as a function of increasing harmonics:

```{r echo = FALSE}
plot(dlm.ts.r2tlb$r2adj, xlab = "Harmonics",ylab = "Adj. R-squared")
plot(dlm.ts.r2tlb$bic, xlab = "Harmonics",ylab = "BIC")
plot(dlm.ts.r2tlb$aic, xlab = "Harmonics",ylab = "AIC")
```

*Harmonics vs. Fit Metrics*

AIC does not minimize in this region and adjusted R-squared continues to increase. BIC has a minimum at 65 harmonics.  The following is the fit with 65 harmonics:

```{r echo = FALSE}
#Plotting the confidence and prediction intervals
predint =  predict(dlm.ts7,df.ts.modelset.harmonic2,interval="prediction")
confint =  predict(dlm.ts7,df.ts.modelset.harmonic2,interval="confidence")
predlower = predint[,2]
predupper = predint[,3]
conflower = confint[,2]
confupper = confint[,3]

plot(df.ts.modelset.harmonic2$IIPI, col="light grey", xlab = "Time", ylab = "IIPI")
lines(predint[,1],col="red")
abline(v=424,col="blue", lty=5)
lines(predlower,col="orange")
lines(predupper,col="orange")
lines(conflower,col="blue")
lines(confupper,col="blue")
```

*Harmonic Model with 65 harmonics*

And the prediction region zoomed in for clarity:

```{r echo = FALSE}
#Plotting the confidence and prediction intervals
predint =  predict(dlm.ts7,df.ts.modelset.harmonic2[400:433,],interval="prediction")
confint =  predict(dlm.ts7,df.ts.modelset.harmonic2[400:433,],interval="confidence")
predlower = predint[,2]
predupper = predint[,3]
conflower = confint[,2]
confupper = confint[,3]

plot(df.ts.modelset.harmonic2$IIPI[400:433], col="black",ylim = c(50,170), xlab = "Time", ylab = "IIPI")
lines(predint[,1],col="red")
abline(v=24,col="blue", lty=5)
lines(predlower,col="orange")
lines(predupper,col="orange")
lines(conflower,col="blue")
lines(confupper,col="blue")
```

*Harmonic Model with 65 harmonics*

The 65 harmonic model appears to still overfit the data, therefore instead in an effort to balance the bias-variance trade-off the visual elbow method is used with the adjusted R-squared metric.

Visual inspection provides that there are about 15 harmonics before there is a reduced rate of increase in adjusted R-squared. Therefore the 15 harmonic model is estimated:

```{r echo = FALSE}
dlm.ts6.summary
```

The Adjusted R-squared is still very high in-sample fit, (0.9784). 

Terms for this model are assessed for pair-wise significance. Only two trigonomtric terms (not of the same harmonic) are not significant.  Therefore this model is retained for comparison going forward.

```{r echo = FALSE}
which(dlm.ts6.summary$coefficients[,'Pr(>|t|)']>0.05) %>% names()
```

The model with 15 harmonics is shown below:

```{r echo = FALSE}
#Plotting the confidence and prediction intervals
predint =  predict(dlm.ts6,df.ts.modelset.harmonic1,interval="prediction")
confint =  predict(dlm.ts6,df.ts.modelset.harmonic1,interval="confidence")
predlower = predint[,2]
predupper = predint[,3]
conflower = confint[,2]
confupper = confint[,3]

plot(df.ts.modelset.harmonic1$IIPI, col="lightgray", xlab = "Time", ylab = "IIPI")
lines(predint[,1],col="red")
abline(v=424,col="blue", lty=5)
lines(predlower,col="orange")
lines(predupper,col="orange")
lines(conflower,col="blue")
lines(confupper,col="blue")
```

*Harmonic Model with 15 Harmonics*

```{r echo = FALSE}
#Plotting the confidence and prediction intervals
predint =  predict(dlm.ts6,df.ts.modelset.harmonic1[400:433,],interval="prediction")
confint =  predict(dlm.ts6,df.ts.modelset.harmonic1[400:433,],interval="confidence")
predlower = predint[,2]
predupper = predint[,3]
conflower = confint[,2]
confupper = confint[,3]

plot(df.ts.modelset.harmonic1$IIPI[400:433], col="black", xlab = "Time", ylab = "IIPI")
lines(predint[,1],col="red")
abline(v=24,col="blue", lty=5)
lines(predlower,col="orange")
lines(predupper,col="orange")
lines(conflower,col="blue")
lines(confupper,col="blue")
```

*Harmonic Model with 15 Harmonics - Prediction Region*

### Residual Assumptions (model with 15 harmonics)

#### Residual Autocorrelation

The Box-Pierce test is performed to identify if residuals are autocorrelated for the model with 15 harmonics (hypothesis defined above): 

```{r echo = FALSE}
Box.test(rstandard(dlm.ts6), lag=36)
```

```{r echo = FALSE}
acf(rstandard(dlm.ts6))
```

The residuals are stationary but not white noise, with fast sinusoidal decay.

# Stochastic Models

## ARIMA Models

Next, Stochstic Models are estimated.

Before fitting a stochastic model the series must be stationary. IIPI is not stationary, but the first difference results in stationarity with sinusoidal decay, as seen above.

Having acheived stationarity, at the first difference, the ACF and PACF are reviewed to determine appropriate ARIMA model parameters:

```{r echo = FALSE}
acf(diff(df.ts.modelset$IIPI))
pacf(diff(df.ts.modelset$IIPI))
```

In reviewing the ACF and PACF there are three proposed reasonable ARIMA models for investigation:

- ARIMA(2,1,0) due to perceived cutoff of the PACF at 2, and interpretable sinusoidal decay of the ACF.

- ARIMA(0,1,5) due to perceived cutoff of the ACF at 5, and interpretable sinusoidal decay of the PACF.

- ARIMA(x,1,y) due to the perceived exponential decay of both ACF and PACF; $x,y = {0,1,2}$

This assessment is validated using a subset search of p and q values (ARIMA(p,1,q)).  The resulting model AICs are displayed:

```{r echo=FALSE, warning = FALSE}
ggplot(arima_table1, aes(x = p_index, y = q_index, fill = arima_aic)) + 
  geom_tile(color = "black") + 
  scale_fill_gradientn(colors = hcl.colors(20,"RdYlGn")) +
  scale_x_discrete(name = "p_index", limits = c(0:10)) + scale_y_discrete(name = "q_index", limits = c(0:10))
```

*ARIMA model AIC Heatmap*

The listed models resulted in the lowest AIC values, verifying the intuition and methodological approach:

```{r echo=FALSE}
kable(arima_table1 %>% arrange(arima_aic) %>% head(3),"simple")
```

All three are estimated and reviewed for comparison given their comparative AICs.

```{r echo=FALSE}
arima210
arima111
arima015
```

As can be seen above, all coefficients are signifcant for each model except $\theta_3$ and $\theta_4$ of the MA(5) model.  Therefore this one is dropped for comparison going forward.  It also, of the three models, has the highest AIC.

The ARIMA(2,1,0) and ARIMA(1,1,1) equations follow:

ARIMA(2,1,0):

$z  = IIPI_{t} - IIPI_{t-1}$

$z_{t} = \phi_{1}z_{t-1} +  \phi_{2}z_{t-2}  + \epsilon_{t}$

ARIMA(1,1,1):

$z_{t} = \phi_{1}z_{t-1} + \epsilon_{t} - \theta_{1}\epsilon_{t-1}$

### Model Fit Plots

```{r echo = FALSE}
plot(df.ts.modelset$IIPI, col="lightgray", xlab = "Time", ylab = "IIPI")
title("ARIMA(2,1,0)")
lines(pred.arima210nahead,col="red")
abline(v=424,col="blue", lty=5)
```

*ARIMA(2,1,0) Model with 20 step ahead predictions*

The model performs poorly when used to predict 10 values into the future.

However, when looking at sequential prediction, the model does incredibly well in the prediction region:

```{r echo = FALSE}
plot(df.ts.modelset$IIPI[424:433], col="black", xlab = "Time", ylab = "IIPI")
title("ARIMA(2,1,0) sequential")
lines(pred.arima2101step,col="red")
abline(v=424,col="blue", lty=5)
```

*ARIMA(2,1,0) Model with sequential prediction*

Similarly for ARIMA(1,1,1):

```{r echo = FALSE}
plot(df.ts.modelset$IIPI, col="lightgray", xlab = "Time", ylab = "IIPI")
title("ARIMA(1,1,1)")
lines(pred.arima111nahead,col="red")
abline(v=424,col="blue", lty=5)
```

*ARIMA(1,1,1) Model with 20 step ahead predictions*

```{r echo = FALSE}
plot(df.ts.modelset.test$IIPI, col="black", xlab = "Time", ylab = "IIPI")
title("ARIMA(1,1,1) sequential")
lines(pred.arima1111step,col="red")
abline(v=424,col="blue", lty=5)

```

*ARIMA(1,1,1) Model with sequential prediction*

### Confirming White Noise

The residuals are reviewed for white noise.

```{r echo = FALSE}
Box.test(arima210$residuals, lag=20)
```

```{r echo = FALSE}
Box.test(arima111$residuals, lag=20)
```

Both are marginally white noise based on the above Box-Pierce tests.  Success! But these models can be improved by stochastic residual correction outlined below.

# Stoachastic Residual Corrections

The following models are reviewed in this section to assess residual correction using ARIMA on the respective resulting residuals:

- Regression Model

- Harmonic Model with 15 harmonics

The Polynomial Model with $k=14$ is not included in this section due to computational restrictions in the approximation methods used to estimate ARIMA models.  Reciprocals are taken of the various polynomial terms which result in computational singularities. For example $t^{14}$ at index $t=423$ results in a value greater that $10^{36}$. The reciprocal is interpreted as singularity by approximation.

## Stochastic Residual Correction for Regression Model

The regression model is:

$IIPI(`r names(mlm.ts2$coefficients)[2:length(names(mlm.ts2$coefficients))] %>% paste(sep = " ")`) = \hat{\beta}_0 + \hat{\beta}_1 TBillCLose + \hat{\beta}_3 InfRate + \hat{\beta}_5 M2 + \hat{\beta}_6 UnempRate + \hat{\beta}_7 Recession + \epsilon_t$

And the residuals of this model were not stationary, therefore the first difference is taken to induce stationarity:

```{r echo = FALSE}
acf(diff(mlm.ts4$residuals))
```

Next the PACF is investigated for ARIMA parameter assignment:

```{r echo = FALSE}
pacf(diff(mlm.ts4$residuals))
```

As both appear potentially cuff-off or fast sinusoidal decay a search is again performed for reasonable $p$ and $q$ values:

```{r echo=FALSE, warning=FALSE}
ggplot(arima_table3 %>% dplyr::filter(p_index != 0), aes(x = p_index, y = q_index, fill = arima_aic)) + 
  geom_tile(color = "black") + 
  scale_fill_gradientn(colors = hcl.colors(20,"RdYlGn")) + 
  coord_fixed()+
  scale_x_discrete(name = "p_index", limits = c(0:10)) + scale_y_discrete(name = "q_index", limits = c(0:3))
```

*ARIMA model AIC Heatmap*

As seen in the above chart, the minimum AIC occurs at $p=3$, $q=0$. Therefore an ARIMA(3,0,0) is fit to the residuals.

```{r echo=FALSE}
kable(arima_table3 %>% arrange(arima_aic) %>% head(3),"simple")
```

### Model Fit Plot

Again, n ahead performs poorly:

```{r echo = FALSE}
plot(df.ts.modelset$IIPI, col="lightgray", xlab = "Time", ylab = "IIPI")
title("Regression with ARIMA(3,0,0) n ahead")
lines(pred.mlm.ts2.arima300nahead,col="red")
abline(v=424,col="blue", lty=5)
```

*Regression with ARIMA(3,0,0) n ahead*

However as seen before, when looking at one step ahead prediction values, the prediction clearly performs much better:

```{r echo = FALSE}
plot(df.ts.modelset$IIPI[424:433], col="black", xlab = "Time", ylab = "IIPI")
title("Regression with ARIMA(3,0,0) sequential")
lines(pred.mlm.ts2.arima3001step[424:433],col="red")
abline(v=424,col="blue", lty=5)

```

*Regression with ARIMA(3,0,0) sequential - prediction region*

### Model Summary

```{r echo = FALSE}
mlm.ts2.arima300
```

All the regression coefficients become insignificant except the intercept and InfRate.

### Confirming White Noise

```{r echo = FALSE}
acf(mlm.ts2.arima300$residuals)
Box.test(mlm.ts2.arima300$residuals)
```

The residuals of the regression model with ARMIA residual correction are white noise.

## Stochastic Residual Correction for Harmonic Model

The harmonic model is ($i \in \{1:15\}$:

$IIPI(t) = \hat{\beta}_0 + \hat{\beta}_1 t + \sum{\hat{\alpha_i}sin(\frac{2\pi i}{s}t)} + \sum{\hat{\beta}_icos(\frac{2\pi i}{s}t)} + \epsilon_t$

First, as a reminder the residuals were very clearly auto-correlated, but already stationary:

```{r echo = FALSE}
acf(dlm.ts6$residuals)
```

The PACF is investigated to determine the appropriate ARIMA model:

```{r echo = FALSE}
pacf(dlm.ts6$residuals)
```

Both the ACF and PACF do not have a clear cut-off, and could be interpreted as showing exponential decay.   Expectation is therefore that an ARMA(p,q) with $p,q \notin {0}$, will likely be applicable for the residual correction.  

```{r echo=FALSE, warning=FALSE}
ggplot(arima_table2, aes(x = p_index, y = q_index, fill = arima_aic)) + 
  geom_tile(color = "black") + 
  scale_fill_gradientn(colors = hcl.colors(20,"RdYlGn")) + 
  coord_fixed()+
  scale_x_discrete(name = "p_index", limits = c(0:10)) + scale_y_discrete(name = "q_index", limits = c(0:10))
```

*ARIMA Model AIC Heatmap*

```{r echo=FALSE}
kable(arima_table2 %>% arrange(arima_aic) %>% head(3),"simple")
```

As seen in the above chart ARIMA(2,0,1) best fits the residuals.

### Model Fit Plot

```{r echo = FALSE}
plot(df.ts.modelset$IIPI, col="lightgray", xlab = "Time", ylab = "IIPI")
title("15 harmonics with ARIMA(2,0,1) n ahead")
lines(pred.dlm.ts6.arima201nahead,col="red")
abline(v=424,col="blue", lty=5)
```

*15 harmonics with ARIMA(2,0,1) n ahead*

Again, as expected the n ahead prediction performs poorly.

```{r echo = FALSE}
plot(df.ts.modelset$IIPI[424:433], col="black", xlab = "Time", ylab = "IIPI")
title("15 harmonics with ARIMA(2,0,1) sequential")
lines(pred.dlm.ts6.arima2011step[424:433],col="red")
abline(v=424,col="blue", lty=5)
```

*15 harmonics with ARIMA(2,0,1) sequential*

... and the sequential prediction performs well.

### Model Summary

```{r echo = FALSE}
dlm.ts6.arima201
```

Only one pair of harmonic terms (sin22 and cos22) have been made insignificant by this addition.

The AIC has been improved by correcting the residuals:

Harmonic Model: `r AIC(dlm.ts6)`

Harmonic Model with residual ARIMA correction: `r dlm.ts6.arima201$aic`

### Confirming White Noise

```{r echo = FALSE}
acf(dlm.ts6.arima201$residuals)
Box.test(dlm.ts6.arima201$residuals, lag=20)
```

Despite the correction, residuals of the model do not result in white noise.

## Generalized Autoregressive conditionally Heteroscedastic Model (GARCH)

The following section investigates using GARCH.  First, as shown before the series and its first difference are not white noise and therefore using GARCH on its own is not applicable.  However, many of the previous models resulted in white noise residuals, and are investigated here.

The following models thus far have resulted in white noise residuals making them candidates for assessing utilization of GARCH:

- ARIMA(2,1,0)

- ARIMA(1,1,1)

- Regression with ARIMA(3,0,0) on the residuals

## ARIMA Models with (GARCH)

### ARIMA(2,1,0) with GARCH

Residuals are first checked for GARCH model potential:

```{r echo=FALSE}
acf(arima210$residuals)
Box.test(arima210$residuals)

acf(arima210$residuals^2)
Box.test(arima210$residuals^2)
```

As can be seen the original ARIMA(2,1,0) model residuals are white noise, but the squared residuals are not.  This is an excellent candidate for using GARCH!

The PACF is displayed to investigate the possible GARCH order.

```{r echo=FALSE}
pacf(arima210$residuals^2)
```

### Model Summary

As a model was not immediately obvious from the ACF and PACF, a appropriate GARCH model was found by estimating a diversity of GARCH parameters using the following search code:

```{r eval = FALSE}
p_index <- list()
q_index <- list()
d <- diff(df.ts.modelset.train$IIPI)
garch_list_2 <- 
  foreach(i=1:2) %:%
  foreach(j=0:2) %do% {
    garch_list<-garchFit(substitute(~arma(2,0)+garch(p,q),list(p=i,q=j)),data=d, trace=FALSE, include.mean = FALSE) 
  }

garch_list_2
```

The output is not included here for brevity, but was reviewed for best AIC, resulting in: ARIMA(2,1,0) with  GARCH(1,1):

```{r echo=FALSE}
arima210.garch11
```

The mean is removed from the model and all model parameters are significant.

### White noise confirmation

```{r echo=FALSE}
acf(arima210.garch11@residuals/arima210.garch11@sigma.t, main="Residuals")
Box.test(arima210.garch11@residuals/arima210.garch11@sigma.t, lag=20)
```

```{r echo=FALSE}
acf((arima210.garch11@residuals/arima210.garch11@sigma.t)^2, main="Residuals^2")
Box.test((arima210.garch11@residuals/arima210.garch11@sigma.t)^2, lag=20)
```

The residuals and squared residuals of this model are white noise!

The following is the predicted conditional variance of the IIPI first-difference:

```{r echo=FALSE, warning=FALSE}
predict(arima210.garch11, plot = TRUE)
```

As expected the n ahead prediction values tend toward the mean asymptotically:

```{r echo = FALSE}
plot(df.ts.modelset$IIPI, col="light grey", xlab = "Time", ylab = "IIPI")
title("ARIMA(2,1,0) with GARCH(1,1) n ahead")
lines(pred.arima210.garch11nahead,col="red")
abline(v=424,col="blue", lty=5)
```

When taking the predictions 1 step ahead, the model predicts well:

```{r echo = FALSE}
plot(df.ts.modelset$IIPI[424:433], col="light grey", xlab = "Time", ylab = "IIPI")
title("ARIMA(2,1,0) with GARCH(1,1) sequential")
lines(pred.arima210.garch11_1step[423:432],col="red")
abline(v=424,col="blue", lty=5)
```

This result can be seen in the MAPE comparison summarised in the Prediction Comparison section below.

### ARIMA(1,1,1) with GARCH

Residuals are first to checked for GARCH model potential:

```{r echo=FALSE}
acf(arima111$residuals)
Box.test(arima111$residuals)

acf(arima111$residuals^2)
Box.test(arima111$residuals^2)
```

As can be seen, again similar to the ARIMA(2,1,0) model, the ARIMA(1,1,1) residuals are white noise, but the squared residuals are not.  This is an excellent candidate for using GARCH, again!

The PACF is displayed to investigate the possible GARCH order.

```{r echo=FALSE}
pacf(arima111$residuals^2)
```

### Model Summary

As a model was not immediately obvious from the ACF and PACF, a appropriate GARCH model was found by estimating a diversity of GARCH parameters using the following search code:

```{r eval = FALSE}
p_index <- list()
q_index <- list()
d <- diff(df.ts.modelset.train$IIPI)
garch_list_2 <- 
  foreach(i=1:2) %:%
  foreach(j=0:2) %do% {
    garch_list<-garchFit(substitute(~arma(1,1)+garch(p,q),list(p=i,q=j)),data=d, trace=FALSE, include.mean = FALSE) 
  }

garch_list_2
```

The output is not included here for brevity, but was reviewed for best AIC, resulting in: ARIMA(1,1,1) with  GARCH(1,1)

```{r echo=FALSE}
arima111.garch11
```

The insignificant mean is removed from the model and all other model parameters are significant.

### White noise confirmation

```{r echo=FALSE}
acf(arima111.garch11@residuals/arima111.garch11@sigma.t, main="Residuals")
Box.test(arima111.garch11@residuals/arima111.garch11@sigma.t, lag=20)
```

```{r echo=FALSE}
acf((arima111.garch11@residuals/arima111.garch11@sigma.t)^2, main="Residuals^2")
Box.test((arima111.garch11@residuals/arima111.garch11@sigma.t)^2, lag=20)
```

The residuals and squared residuals of this model are white noise!

The following is the predicted conditional variance of the IIPI first-difference:

```{r echo=FALSE}
predict(arima111.garch11, plot = TRUE)
```

As expected the n ahead prediction value tends toward an asymptotic value:

```{r echo = FALSE}
plot(df.ts.modelset$IIPI, col="light grey", xlab = "Time", ylab = "IIPI")
title("ARIMA(1,1,1) with GARCH(1,1) n ahead")
lines(pred.arima111.garch11nahead,col="red")
abline(v=424,col="blue", lty=5)
```

When taking the predictions 1 step ahead, the model predicts well:

```{r echo = FALSE}
plot(df.ts.modelset$IIPI[423:434], col="black", xlab = "Time", ylab = "IIPI")
title("ARIMA(1,1,1) with GARCH(1,1) sequential")
lines(pred.arima111.garch11_1step[422:433],col="red")
abline(v=424,col="blue", lty=5)
```

This result can be seen in the MAPE comparison summarized in the Prediction Comparison section.

### Regression with ARMIA(3,0,0) on residuals and GARCH

Residuals are first checked for GARCH model potential:

```{r echo=FALSE}
acf(mlm.ts2.arima300$residuals)
Box.test(mlm.ts2.arima300$residuals)

acf(mlm.ts2.arima300$residuals^2)
Box.test(mlm.ts2.arima300$residuals^2)
```

As can be seen, similar to the ARIMA(2,1,0) and  ARIMA(1,1,1) models, the residuals of the regression with ARIMA(3,0,0) residuals are white noise, but the squared residuals are not.  This is again an excellent candidate for using GARCH!

The PACF is displayed to investigate the possible GARCH order.

```{r echo=FALSE}
pacf(mlm.ts2.arima300$residuals^2)
```

### Model Summary

As a model was not immediately obvious from the ACF and PACF, a appropriate GARCH model was assumed and estimated using the *rugarch* R library. NOTE:  for this implementation of GARCH, *fGARCH* was not used, as it did not appear to allow for inclusion of external regression parameters:

```{r eval = FALSE}
p_index <- list()
q_index <- list()
d <- diff(df.ts.modelset.train$IIPI) 
x <- df.ts.modelset.train[2:423,] %>% select(-IIPI,-Date_form,-GDP,-IndPro) %>% as.matrix()

spec = ugarchspec(mean.model = list(armaOrder = c(3, 0), external.regressors = cbind(x)))
fit = ugarchfit(spec = spec, data = d)
```

```{r echo = FALSE}
fit
```

To summarize the output above:

mxreg1 is through mxreg5 correspond to `r df.ts.modelset.train[2:423,] %>% select(-IIPI,-Date_form,-GDP,-IndPro) %>% names()`

It appears that with inclusion of the GARCH on the residuals $\phi_2$ and $\phi_3$ are no longer significant, and the only significant regressor is again InfRate (as expected based on the regression with ARIMA(3,0,0) result).  All GARCH coefficients are significant.

### White noise confirmation

```{r echo=FALSE}
acf(fit@fit$residuals, main="Residuals")
Box.test(fit@fit$residuals, lag=20)
```

Interestingly, residuals of this model would not be considered white noise.

As expected the n ahead prediction value tends toward an asymptotic value:

```{r echo=FALSE}
plot(df.ts.modelset$IIPI, col="light grey", xlab = "Time", ylab = "IIPI")
title("Regression with ARIMA(3,0,0) then GARCH(1,1)")
lines(pred.fit,col="red")
abline(v=424,col="blue", lty=5)
```

When taking the predictions 1 step ahead, the model performs much better, though not as well as the ARIMA models above:

```{r echo = FALSE}
plot(df.ts.modelset$IIPI[424:434], col="black", xlab = "Time", ylab = "IIPI")
title("Regression with ARIMA(3,0,0) then GARCH(1,1)")
lines(pred.reg.arima300.garch11_1step[424:434],col="red")
abline(v=424,col="blue", lty=5)
```

## Vector Autoregressive Moving Average

VARMA is used to estimate systems of time series data, allowing for concurent parameter estimation. 

To perform VARMA, first, stationarity is induced in all variables by taking the first difference. Recession is removed from the dataset as it is an indicator variable and is therefore not applicable in VARMA modelling.  All pair-wise cross-correlation functions (CCFs) are checked for bivariate white noise.  If one variable is found to be bivariate white noise with all others, it would be removed from the subsequent VARMA model:

```{r echo = FALSE}
par(mfrow=c(2,2))
ccf(df.ts.modelset.train.vector.diff$IIPI,df.ts.modelset.train.vector.diff$TBillClose, ylab = "CCF", main = "IIPI,TBillClose") # not white noise...
ccf(df.ts.modelset.train.vector.diff$IIPI,df.ts.modelset.train.vector.diff$GDP, ylab = "CCF", main = "IIPI,GDP") # not white noise...
ccf(df.ts.modelset.train.vector.diff$IIPI,df.ts.modelset.train.vector.diff$InfRate, ylab = "CCF", main = "IIPI,InfRate") # not white noise...
ccf(df.ts.modelset.train.vector.diff$IIPI,df.ts.modelset.train.vector.diff$IndPro, ylab = "CCF", main = "IIPI,IndPro") # not white noise...
ccf(df.ts.modelset.train.vector.diff$IIPI,df.ts.modelset.train.vector.diff$M2, ylab = "CCF", main = "IIPI,M2") # not white noise...
ccf(df.ts.modelset.train.vector.diff$IIPI,df.ts.modelset.train.vector.diff$UnempRate, ylab = "CCF", main = "IIPI,UnempRate") # not white noise...
ccf(df.ts.modelset.train.vector.diff$TBillClose,df.ts.modelset.train.vector.diff$GDP, ylab = "CCF", main = "TBillClose,GDP") # not white noise...
ccf(df.ts.modelset.train.vector.diff$TBillClose,df.ts.modelset.train.vector.diff$InfRate, ylab = "CCF", main = "TBillClose,InfRate") # not white noise...
ccf(df.ts.modelset.train.vector.diff$TBillClose,df.ts.modelset.train.vector.diff$IndPro, ylab = "CCF", main = "TBillClose,IndPro") # not white noise...
ccf(df.ts.modelset.train.vector.diff$TBillClose,df.ts.modelset.train.vector.diff$M2, ylab = "CCF", main = "TBillClose,M2") # IS WHITE NOISE
ccf(df.ts.modelset.train.vector.diff$TBillClose,df.ts.modelset.train.vector.diff$UnempRate, ylab = "CCF", main = "TBillClose,UnempRate") # not white noise...
ccf(df.ts.modelset.train.vector.diff$GDP,df.ts.modelset.train.vector.diff$InfRate, ylab = "CCF", main = "GDP,InfRate") # not white noise...
ccf(df.ts.modelset.train.vector.diff$GDP,df.ts.modelset.train.vector.diff$IndPro, ylab = "CCF", main = "GDP,Indrop") # not white noise...
ccf(df.ts.modelset.train.vector.diff$GDP,df.ts.modelset.train.vector.diff$M2, ylab = "CCF", main = "GDP,M2") # not white noise
ccf(df.ts.modelset.train.vector.diff$GDP,df.ts.modelset.train.vector.diff$UnempRate, ylab = "CCF", main = "GDP,UnempRate") # not white noise...
ccf(df.ts.modelset.train.vector.diff$InfRate,df.ts.modelset.train.vector.diff$IndPro, ylab = "CCF", main = "InfRate,IndPro") # not white noise...
ccf(df.ts.modelset.train.vector.diff$InfRate,df.ts.modelset.train.vector.diff$M2, ylab = "CCF", main = "InfRate,M2") # IS WHITE NOISE
ccf(df.ts.modelset.train.vector.diff$InfRate,df.ts.modelset.train.vector.diff$UnempRate, ylab = "CCF", main = "InfRate,UnempRate") # not white noise...
ccf(df.ts.modelset.train.vector.diff$IndPro,df.ts.modelset.train.vector.diff$M2, ylab = "CCF", main = "IndPro,M2") # not white noise
ccf(df.ts.modelset.train.vector.diff$IndPro,df.ts.modelset.train.vector.diff$UnempRate, ylab = "CCF", main = "IndPro,UnempRate") # not white noise...
ccf(df.ts.modelset.train.vector.diff$M2,df.ts.modelset.train.vector.diff$UnempRate, ylab = "CCF", main = "M2,UnempRate") # IS WHITE NOISE
```

M2 would a candidate for removal from the VARMA except that it is not bivariate white noise with GDP.  All others are clearly not bivariate white noise.

In consideration of computation time only VAR(1) model is estimated, with a matrix of 49 $\phi$ estimates:

```{r echo = FALSE}
temp_mat1 <- varma1_0$coef
colnames(temp_mat1) <- df.ts.modelset.train[2:423,] %>% select(-Date_form,-Recession) %>% names()
rownames(temp_mat1) <- df.ts.modelset.train[2:423,] %>% select(-Date_form,-Recession) %>% names()
knitr::kable(temp_mat1, "simple")
```

An interesting note: Industrial Production from the previous month has significant affect on the subsequent month's GDP!

However, only those with a ratio of estimate to standard error > ~1.96 are significant:

```{r echo = FALSE}
temp_mat2 <- abs(varma1_0$coef/varma1_0$secoef)
colnames(temp_mat2) <- df.ts.modelset.train[2:423,] %>% select(-Date_form,-Recession) %>% names()
rownames(temp_mat2) <- df.ts.modelset.train[2:423,] %>% select(-Date_form,-Recession) %>% names()
knitr::kable(temp_mat2, "simple")
```

The vectors of residuals are not white noise:

```{r echo=FALSE}
par(mfrow=c(2,2))
acf(varma1_0$residuals[,1], main = "IIPI")
acf(varma1_0$residuals[,2], main = "TBillClose")
acf(varma1_0$residuals[,3], main = "GDP")
acf(varma1_0$residuals[,4], main = "InfRate")
acf(varma1_0$residuals[,5], main = "IndPro")
acf(varma1_0$residuals[,6], main = "M2")
acf(varma1_0$residuals[,7], main = "UnempRate")
```

### Model Fit

IIPI exclusively, the model fit is:

```{r echo = FALSE}
plot(df.ts.modelset$IIPI[424:434], col="light grey", xlab = "Time", ylab = "IIPI")
title("VAR(1)")
lines(pred.varma1_0.nahead,col="red")
abline(v=424,col="blue", lty=5)
```

Sequential 1 step ahead is not included for comparison, as this requires re-estimating the model ten times in a loop, in consideration of computation time.  However, it is expected that it would perform better than the n ahead ($n =10$), as seen for the other ARIMA models.  VARMA would be no different in this regard.

# Prediction Comparison

The various models outlined above are compared using mean absolute percent error (MAPE).   The 1 step sequential predictions out perform all other methods.   

```{r echo=FALSE, warings=FALSE}

mape <- function(d,p){
  mean(abs((d-p)/d))
}

comparetable <- tibble(
  Model = c(
    'Regression Model, variable reduced',
    'Deterministic Model, Monthly Indicator with Trend',
    'Deterministic Model, Polynomial k=14',
    'Deterministic Model, 100 Harmonics',
    'Deterministic Model, 85 Harmonics',
    'Deterministic Model, 15 Harmonics',
    'Deterministic Model, 65 Harmonics',
    'ARIMA(2,1,0) n ahead',
    'ARIMA(1,1,1) n ahead',
    'ARIMA(0,1,5) n ahead',
    'ARIMA(2,1,0) seq. 1 ahead',
    'ARIMA(1,1,1) seq. 1 ahead',
    'ARIMA(0,1,5) seq. 1 ahead',
    'Regression, residual ARIMA(3,0,0), n ahead',
    'Regression, residual ARIMA(3,0,0), seq. 1 ahead',
    'Deterministic Model, 15 Harmonics, residual ARIMA, n ahead',
    'Deterministic Model, 15 Harmonics, residual ARIMA, seq. 1 ahead',
    'ARIMA(1,1,1) with GARCH(1,1), n ahead',
    'AIMAA(1,1,1) with GARCH(1,1) seq. 1 ahead',
    'VARMA prediction of IIPI',
    'ARIMA(2,1,0) with GARCH(1,1), n ahead',
    'AIMAA(2,1,0) with GARCH(1,1) seq. 1 ahead',
    'Regression, residual ARIMA(3,0,0), GARCH(1,1), n ahead',
    'Regression, residual ARIMA(3,0,0), GARCH(1,1), seq. 1 ahead'),
    MAPE = c(
    mape(d = df.ts.modelset.test$IIPI, 
         p = predict(mlm.ts2, df.ts.modelset.test %>% select(-c(IIPI,Date_form,GDP,IndPro)))),
    mape(d = df.ts.modelset.deterministic.test$IIPI, 
         p = predict(dlm.ts1, df.ts.modelset.deterministic.test)),
    mape(d = df.ts.modelset.deterministic.test$IIPI, 
         p = predict(dlm.ts2, df.ts.modelset.deterministic.test)),
    mape(d = df.ts.modelset.harmonic.test$IIPI, 
         p = predict(dlm.ts3, df.ts.modelset.harmonic.test)),
    mape(d = df.ts.modelset.harmonic.test$IIPI, 
         p = predict(dlm.ts4, df.ts.modelset.harmonic.test)),
    mape(d = df.ts.modelset.harmonic.test1$IIPI, 
         p = predict(dlm.ts6, df.ts.modelset.harmonic.test1)),
    mape(d = df.ts.modelset.harmonic.test2$IIPI, 
         p = predict(dlm.ts7, df.ts.modelset.harmonic.test2)),
    mape(d = df.ts.modelset.test$IIPI, 
         p = predict(arima210, n.ahead =10)$pred),
    mape(d = df.ts.modelset.test$IIPI, 
         p = predict(arima111, n.ahead =10)$pred),    
    mape(d = df.ts.modelset.test$IIPI, 
         p = predict(arima015, n.ahead =10)$pred),
    mape(d = df.ts.modelset.test$IIPI, 
         p = pred.arima2101step),
    mape(d = df.ts.modelset.test$IIPI, 
         p = pred.arima1111step),
    mape(d = df.ts.modelset.test$IIPI, 
         p = pred.arima0151step),
    mape(d = df.ts.modelset.test$IIPI,
         p = pred.mlm.ts2.arima300nahead[424:433]),
    mape(d = df.ts.modelset.test$IIPI,
         p = pred.mlm.ts2.arima3001step[424:433]),
    mape(d = df.ts.modelset.test$IIPI,
         p = pred.dlm.ts6.arima201nahead[424:433]),
    mape(d = df.ts.modelset.test$IIPI,
         p = pred.dlm.ts6.arima2011step[424:433]),
    mape(d = df.ts.modelset.test$IIPI,
         p = pred.arima111.garch11nahead[423:432]),
    mape(d = df.ts.modelset.test$IIPI,
         p = pred.arima111.garch11_1step[423:432]),
    mape(d = df.ts.modelset.test$IIPI, 
         p = pred.varma1_0.nahead),
    mape(d = df.ts.modelset.test$IIPI, 
         p = pred.arima210.garch11nahead[423:432]),
    mape(d = df.ts.modelset.test$IIPI, 
         p = pred.arima210.garch11_1step[423:432]),
    mape(d = df.ts.modelset.test$IIPI, 
         p = pred.fit[423:432]),
    mape(d = df.ts.modelset.test$IIPI, 
         p = pred.reg.arima300.garch11_1step[424:433])
    )
)


kable(comparetable %>% arrange(MAPE), "simple")
```

# Transfer Function

In order to perform Transfer Function modelling, pre-whitening must be performed on all the stationary first-differenced independent variables.  This is accomplished using various ARIMA models on each independent variable.  For each independent variable first differenced ACF and PACF are displayed, then used for assessment of potential ARIMA model, then whie noise is verified using Box-Pierce:

```{r echo = FALSE}
acf(diff(df.ts.modelset.train.vector.diff$TBillClose))
pacf(diff(df.ts.modelset.train.vector.diff$TBillClose))
arimaTBillClose <- arima(df.ts.modelset.train.vector.diff$TBillClose,order = c(2,0,2))
arimaTBillClose
Box.test(arimaTBillClose$residuals)
```

An ARIMA(2,0,2) is used to pre-whiten the first-differenced TBillClose, and the resulting residuals can be seen to be white noise based on the Box-Pierce test.

```{r echo = FALSE}
acf(diff(df.ts.modelset.train.vector.diff$GDP, lag = 3))
pacf(diff(df.ts.modelset.train.vector.diff$GDP))
arimaGDP <- arima(df.ts.modelset.train.vector.diff$GDP,order = c(1,0,1))
arimaGDP
Box.test(arimaGDP$residuals)
```

An ARIMA(1,0,1) is used to pre-whiten the first-differenced TBillClose, and the resulting residuals can be seen to be white noise based on the Box-Pierce test.

```{r echo = FALSE}
acf(diff(df.ts.modelset.train.vector.diff$InfRate))
pacf(diff(df.ts.modelset.train.vector.diff$InfRate))
arimaInfRate <- arima(df.ts.modelset.train.vector.diff$InfRate,order = c(2,0,0), seasonal=list(order=c(1,0,1), period=12))
arimaInfRate
Box.test(arimaInfRate$residuals)
```

A SARIMA(2,0,0)(1,0,1)$_{12}$ is used to pre-whiten the first-differenced InfRate, and the resulting residuals can be seen to be white noise based on the Box-Pierce test.

```{r echo = FALSE}
acf(diff(df.ts.modelset.train.vector.diff$IndPro))
pacf(diff(df.ts.modelset.train.vector.diff$IndPro))
arimaIndPro <- arima(df.ts.modelset.train.vector.diff$IndPro,order = c(2,0,2))
arimaIndPro
Box.test(arimaIndPro$residuals)
```

An ARIMA(2,0,2) is used to pre-whiten the first-differenced IndPro, and the resulting residuals can be seen to be white noise based on the Box-Pierce test.

```{r echo = FALSE}
acf(diff(df.ts.modelset.train.vector.diff$M2))
pacf(diff(df.ts.modelset.train.vector.diff$M2))
arimaM2 <- arima(df.ts.modelset.train.vector.diff$M2,order = c(1,0,0), seasonal=list(order=c(1,0,0), period=12))
arimaM2
Box.test(arimaM2$residuals)
```

A SARIMA(1,0,0)(1,0,0)$_{12}$ is used to pre-whiten the first-differenced M2, and the resulting residuals can be seen to be white noise based on the Box-Pierce test.

```{r echo = FALSE}
acf(diff(df.ts.modelset.train.vector.diff$UnempRate))
pacf(diff(df.ts.modelset.train.vector.diff$UnempRate))
arimaUnempRate <- arima(df.ts.modelset.train.vector.diff$UnempRate,order = c(2,1,2))
arimaUnempRate
Box.test(arimaUnempRate$residuals)
```

An ARIMA(2,0,2) is used to pre-whiten the first-differenced UnempRate, and the resulting residuals can be seen to be white noise based on the Box-Pierce test.

Next the CCFs are displayed to determine $b$, $r$, $s$ values:

```{r echo = FALSE}
par(mfrow=c(2,2))
ccf(df.ts.modelset.train.vector.prewhite$IIPI, df.ts.modelset.train.vector.prewhite$TBillClose, ylab = "CCF", main = "IIPI,TBillClose")
ccf(df.ts.modelset.train.vector.prewhite$IIPI, df.ts.modelset.train.vector.prewhite$GDP, ylab = "CCF", main = "IIPI,GDP")
ccf(df.ts.modelset.train.vector.prewhite$IIPI, df.ts.modelset.train.vector.prewhite$InfRate, ylab = "CCF", main = "IIPI,InfRate")
ccf(df.ts.modelset.train.vector.prewhite$IIPI, df.ts.modelset.train.vector.prewhite$IndPro, ylab = "CCF", main = "IIPI,IndPro")
ccf(df.ts.modelset.train.vector.prewhite$IIPI, df.ts.modelset.train.vector.prewhite$M2, ylab = "CCF", main = "IIPI,M2")
ccf(df.ts.modelset.train.vector.prewhite$IIPI, df.ts.modelset.train.vector.prewhite$UnempRate, ylab = "CCF", main = "IIPI,UnempRate")
```

## Transfer Function on IndPro

Taking IndPro as the independent variable, and $b = 2$, $r = 1$, and $s = 0$.

```{r eval = FALSE}
X_b2 <- df.ts.modelset.train.vector.prewhite$IndPro[1:421]- mean(df.ts.modelset.train.vector.prewhite$IndPro)
Y_b2 <- df.ts.modelset.train.vector.prewhite$IIPI[3:423] - mean(df.ts.modelset.train.vector.prewhite$IIPI)
```

The transfer function is estimated, forcing the residual correction to zero first to investigate:

```{r eval = FALSE} 
m1<-arimax(Y_b2, order=c(1,0,0), fixed=c(0,NA,NA), xtransf=data.frame(X_b2), transfer=list(c(1,0)), include.mean = FALSE)
```

```{r echo = FALSE}
m1
```

The model coefficients are significant, based on the ratio of estimates to standard error.

```{r echo = FALSE}
par(mfrow=c(1,2))
acf(m1$residuals, na.action = na.pass)
pacf(m1$residuals, na.action = na.pass)
Box.test(m1$residuals)
```

However, the residuals of the model are not white noise and seen in the above Box-Pierce and ACF. This is corrected using $ARIMA(1,0,1)$:

```{r echo = FALSE}
m_arima11_resid
```

```{r echo = FALSE}
Box.test(m_arima11_resid$residuals)
```

The transfer function model of IIPI using IndPro lagged by two months as predictor with ARIMA(1,0,1) correction on the residuals results in white noise.  The model estimated is:

$IIPI_t = \frac{\omega_0}{1 - \delta_1B}IndPro_{t-2} + \epsilon_t$ 

# Conclusion

Time series analysis is an exercise in extracting information from current and previous values of variables of interest and other predictors to allow explanation of the current or future values of a random variable.  ARIMA takes into account previous values of the dependent variable.  Transfer functions take into account previous values of independent variables.  Regression models take into account the current values of independent variables.  

It is my hope that this project outlines the various strenghts of these methods and provides clear overview of the methods and thought processes undertaken to enact each model.

The following Models resulted in white noise:

- ARIMA(2,1,0)

- ARIMA(1,1,1)

- Regression with ARMIA(3,0,0)

- ARIMA(2,1,0) with GARCH(1,1)

- ARIMA(1,1,1) with GARCH(1,1)

Key project takeaways:

- The choice of independent variables were unfortunately lacking in predictive value.

- Harmonic models can be made to overfit in-sample, but as soon as prediction is involved, the bias-variance trade off has to be accounted for.

- Inducing white noise using ARIMA could typically be accomplished using arbitrarily many p or q values but at the cost of more parameters to estimate, and compromise of model interpretability and parsimony. 

- N step ahead predictions are very poor typically beyond 1 step.

- Sequential prediction (1 step ahead) is much better.

- A transfer function with s and r equal to zero is simply a regression model. It was noted that inflation rate could have been used in transfer function modelling but the $s$ and $r$ parameters would have been zero, resulting in a trivial regression model.

























